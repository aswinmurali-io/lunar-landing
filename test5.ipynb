{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gym\n",
    "# ! pip3 install box2d-py\n",
    "# ! pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz\n",
    "# ! pip install 'ribs[all]' gym~=0.17.0 Box2D~=2.3.10 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=1000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "            new_s, r, done, info = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    " \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "\n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.387, reward_mean=-189.4,reward_threshold=-106.3\n",
      "1: loss=1.381, reward_mean=-169.1,reward_threshold=-87.3\n",
      "2: loss=1.366, reward_mean=-176.7,reward_threshold=-94.5\n",
      "3: loss=1.342, reward_mean=-166.7,reward_threshold=-95.0\n",
      "4: loss=1.299, reward_mean=-163.3,reward_threshold=-110.7\n",
      "5: loss=1.242, reward_mean=-181.4,reward_threshold=-115.1\n",
      "6: loss=1.255, reward_mean=-136.0,reward_threshold=-98.3\n",
      "7: loss=1.250, reward_mean=-142.4,reward_threshold=-101.4\n",
      "8: loss=1.238, reward_mean=-148.5,reward_threshold=-97.9\n",
      "9: loss=1.271, reward_mean=-130.6,reward_threshold=-84.0\n",
      "10: loss=1.275, reward_mean=-127.4,reward_threshold=-92.0\n",
      "11: loss=1.285, reward_mean=-147.6,reward_threshold=-86.6\n",
      "12: loss=1.286, reward_mean=-153.0,reward_threshold=-84.5\n",
      "13: loss=1.279, reward_mean=-124.5,reward_threshold=-88.6\n",
      "14: loss=1.286, reward_mean=-127.1,reward_threshold=-85.6\n",
      "15: loss=1.290, reward_mean=-128.7,reward_threshold=-72.5\n",
      "16: loss=1.264, reward_mean=-158.6,reward_threshold=-75.5\n",
      "17: loss=1.242, reward_mean=-127.5,reward_threshold=-67.1\n",
      "18: loss=1.239, reward_mean=-105.3,reward_threshold=-53.6\n",
      "19: loss=1.192, reward_mean=-113.0,reward_threshold=-60.1\n",
      "20: loss=1.174, reward_mean=-109.9,reward_threshold=-61.6\n",
      "21: loss=1.180, reward_mean=-96.1,reward_threshold=-34.8\n",
      "22: loss=1.104, reward_mean=-91.4,reward_threshold=-31.5\n",
      "23: loss=1.090, reward_mean=-98.0,reward_threshold=-20.7\n",
      "24: loss=1.160, reward_mean=-107.7,reward_threshold=-16.6\n",
      "25: loss=1.092, reward_mean=-100.5,reward_threshold=-29.3\n",
      "26: loss=1.015, reward_mean=-76.2,reward_threshold=-4.3\n",
      "27: loss=1.074, reward_mean=-30.2,reward_threshold=16.8\n",
      "28: loss=1.023, reward_mean=-75.1,reward_threshold=-12.0\n",
      "29: loss=1.041, reward_mean=-66.9,reward_threshold=10.1\n",
      "30: loss=1.040, reward_mean=-22.2,reward_threshold=27.5\n",
      "31: loss=1.045, reward_mean=-10.8,reward_threshold=24.9\n",
      "32: loss=1.051, reward_mean=-13.5,reward_threshold=10.5\n",
      "33: loss=1.048, reward_mean=-13.2,reward_threshold=13.6\n",
      "34: loss=0.898, reward_mean=2.8,reward_threshold=28.2\n",
      "35: loss=1.067, reward_mean=12.9,reward_threshold=37.0\n",
      "36: loss=0.985, reward_mean=10.2,reward_threshold=34.1\n",
      "37: loss=0.953, reward_mean=13.7,reward_threshold=41.9\n",
      "38: loss=0.892, reward_mean=16.5,reward_threshold=45.3\n",
      "39: loss=0.880, reward_mean=14.9,reward_threshold=55.2\n",
      "40: loss=0.796, reward_mean=7.1,reward_threshold=61.4\n",
      "41: loss=0.694, reward_mean=9.0,reward_threshold=85.4\n",
      "42: loss=0.616, reward_mean=14.0,reward_threshold=186.9\n",
      "43: loss=0.569, reward_mean=-10.2,reward_threshold=52.4\n",
      "44: loss=0.605, reward_mean=-49.9,reward_threshold=-3.6\n",
      "45: loss=0.580, reward_mean=-67.2,reward_threshold=-6.8\n",
      "46: loss=0.668, reward_mean=-36.7,reward_threshold=4.4\n",
      "47: loss=0.667, reward_mean=-1.7,reward_threshold=27.1\n",
      "48: loss=0.678, reward_mean=23.8,reward_threshold=41.1\n",
      "49: loss=0.637, reward_mean=27.9,reward_threshold=51.6\n",
      "50: loss=0.573, reward_mean=30.5,reward_threshold=76.5\n",
      "51: loss=0.632, reward_mean=16.0,reward_threshold=189.5\n",
      "52: loss=0.648, reward_mean=-8.7,reward_threshold=171.4\n",
      "53: loss=0.709, reward_mean=-72.7,reward_threshold=-22.4\n",
      "54: loss=0.808, reward_mean=-134.3,reward_threshold=-84.1\n",
      "55: loss=0.829, reward_mean=-154.6,reward_threshold=-92.6\n",
      "56: loss=0.853, reward_mean=-156.8,reward_threshold=-100.2\n",
      "57: loss=0.890, reward_mean=-146.0,reward_threshold=-88.9\n",
      "58: loss=0.669, reward_mean=-105.5,reward_threshold=-57.3\n",
      "59: loss=0.670, reward_mean=-60.7,reward_threshold=-37.3\n",
      "60: loss=0.538, reward_mean=-18.4,reward_threshold=16.2\n",
      "61: loss=0.613, reward_mean=-28.5,reward_threshold=-10.9\n",
      "62: loss=0.580, reward_mean=-7.2,reward_threshold=4.5\n",
      "63: loss=0.604, reward_mean=7.3,reward_threshold=25.6\n",
      "64: loss=0.585, reward_mean=20.3,reward_threshold=36.6\n",
      "65: loss=0.594, reward_mean=26.2,reward_threshold=44.4\n",
      "66: loss=0.570, reward_mean=30.9,reward_threshold=52.8\n",
      "67: loss=0.492, reward_mean=21.4,reward_threshold=61.4\n",
      "68: loss=0.485, reward_mean=-28.1,reward_threshold=63.9\n",
      "69: loss=0.545, reward_mean=-82.6,reward_threshold=-23.2\n",
      "70: loss=0.679, reward_mean=-131.6,reward_threshold=-83.0\n",
      "71: loss=0.675, reward_mean=-146.9,reward_threshold=-83.2\n",
      "72: loss=0.452, reward_mean=-65.4,reward_threshold=25.8\n",
      "73: loss=0.545, reward_mean=-41.4,reward_threshold=42.4\n",
      "74: loss=0.485, reward_mean=30.6,reward_threshold=220.9\n",
      "75: loss=0.490, reward_mean=38.2,reward_threshold=216.6\n",
      "76: loss=0.514, reward_mean=45.7,reward_threshold=235.3\n",
      "77: loss=0.453, reward_mean=19.3,reward_threshold=189.6\n",
      "78: loss=0.458, reward_mean=42.8,reward_threshold=214.0\n",
      "79: loss=0.448, reward_mean=71.0,reward_threshold=224.7\n",
      "80: loss=0.466, reward_mean=35.1,reward_threshold=210.8\n",
      "81: loss=0.471, reward_mean=56.1,reward_threshold=210.2\n",
      "82: loss=0.467, reward_mean=-6.8,reward_threshold=188.8\n",
      "83: loss=0.456, reward_mean=50.0,reward_threshold=222.4\n",
      "84: loss=0.408, reward_mean=43.8,reward_threshold=208.5\n",
      "85: loss=0.498, reward_mean=55.4,reward_threshold=211.0\n",
      "86: loss=0.464, reward_mean=47.2,reward_threshold=206.0\n",
      "87: loss=0.454, reward_mean=43.6,reward_threshold=205.2\n",
      "88: loss=0.454, reward_mean=16.4,reward_threshold=178.6\n",
      "89: loss=0.495, reward_mean=8.8,reward_threshold=190.7\n",
      "90: loss=0.459, reward_mean=69.6,reward_threshold=227.1\n",
      "91: loss=0.465, reward_mean=67.3,reward_threshold=234.8\n",
      "92: loss=0.435, reward_mean=31.5,reward_threshold=225.2\n",
      "93: loss=0.499, reward_mean=11.2,reward_threshold=149.0\n",
      "94: loss=0.450, reward_mean=-2.3,reward_threshold=171.2\n",
      "95: loss=0.453, reward_mean=38.7,reward_threshold=206.7\n",
      "96: loss=0.428, reward_mean=28.7,reward_threshold=196.6\n",
      "97: loss=0.438, reward_mean=56.1,reward_threshold=225.4\n",
      "98: loss=0.448, reward_mean=36.2,reward_threshold=218.6\n",
      "99: loss=0.423, reward_mean=65.6,reward_threshold=241.9\n",
      "100: loss=0.415, reward_mean=48.7,reward_threshold=232.1\n",
      "101: loss=0.469, reward_mean=75.9,reward_threshold=242.5\n",
      "102: loss=0.478, reward_mean=81.3,reward_threshold=237.1\n",
      "103: loss=0.418, reward_mean=75.7,reward_threshold=246.7\n",
      "104: loss=0.452, reward_mean=61.7,reward_threshold=233.6\n",
      "105: loss=0.449, reward_mean=45.9,reward_threshold=230.6\n",
      "106: loss=0.440, reward_mean=95.6,reward_threshold=239.3\n",
      "107: loss=0.480, reward_mean=99.9,reward_threshold=245.4\n",
      "108: loss=0.430, reward_mean=66.9,reward_threshold=235.3\n",
      "109: loss=0.457, reward_mean=93.0,reward_threshold=241.4\n",
      "110: loss=0.448, reward_mean=92.6,reward_threshold=231.4\n",
      "111: loss=0.414, reward_mean=134.2,reward_threshold=247.5\n",
      "112: loss=0.466, reward_mean=134.6,reward_threshold=250.2\n",
      "113: loss=0.457, reward_mean=143.7,reward_threshold=249.2\n",
      "114: loss=0.422, reward_mean=120.0,reward_threshold=243.7\n",
      "115: loss=0.476, reward_mean=124.4,reward_threshold=245.1\n",
      "116: loss=0.527, reward_mean=150.8,reward_threshold=249.2\n",
      "117: loss=0.570, reward_mean=80.2,reward_threshold=229.9\n",
      "118: loss=0.590, reward_mean=73.1,reward_threshold=222.2\n",
      "119: loss=0.577, reward_mean=88.4,reward_threshold=232.4\n",
      "120: loss=0.611, reward_mean=73.1,reward_threshold=217.9\n",
      "121: loss=0.638, reward_mean=48.8,reward_threshold=220.8\n",
      "122: loss=0.628, reward_mean=64.1,reward_threshold=202.0\n",
      "123: loss=0.679, reward_mean=58.3,reward_threshold=169.0\n",
      "124: loss=0.713, reward_mean=53.6,reward_threshold=135.4\n",
      "125: loss=0.672, reward_mean=41.3,reward_threshold=128.3\n",
      "126: loss=0.731, reward_mean=49.1,reward_threshold=143.3\n",
      "127: loss=0.716, reward_mean=36.5,reward_threshold=143.2\n",
      "128: loss=0.747, reward_mean=35.9,reward_threshold=141.6\n",
      "129: loss=0.784, reward_mean=20.7,reward_threshold=107.3\n",
      "130: loss=0.898, reward_mean=47.4,reward_threshold=130.8\n",
      "131: loss=0.933, reward_mean=19.4,reward_threshold=110.1\n",
      "132: loss=0.975, reward_mean=19.8,reward_threshold=105.9\n",
      "133: loss=1.051, reward_mean=13.6,reward_threshold=112.4\n",
      "134: loss=1.059, reward_mean=33.1,reward_threshold=119.4\n",
      "135: loss=1.072, reward_mean=25.3,reward_threshold=110.4\n",
      "136: loss=1.104, reward_mean=25.4,reward_threshold=106.6\n",
      "137: loss=1.104, reward_mean=30.3,reward_threshold=120.0\n",
      "138: loss=1.104, reward_mean=48.6,reward_threshold=126.0\n",
      "139: loss=1.116, reward_mean=49.6,reward_threshold=125.3\n",
      "140: loss=1.082, reward_mean=52.3,reward_threshold=132.9\n",
      "141: loss=1.068, reward_mean=61.4,reward_threshold=135.9\n",
      "142: loss=1.043, reward_mean=74.0,reward_threshold=148.8\n",
      "143: loss=1.020, reward_mean=88.7,reward_threshold=158.7\n",
      "144: loss=1.027, reward_mean=89.1,reward_threshold=157.1\n",
      "145: loss=1.023, reward_mean=83.0,reward_threshold=152.1\n",
      "146: loss=0.987, reward_mean=81.0,reward_threshold=154.1\n",
      "147: loss=0.933, reward_mean=81.4,reward_threshold=158.1\n",
      "148: loss=0.856, reward_mean=100.8,reward_threshold=166.5\n",
      "149: loss=0.778, reward_mean=101.5,reward_threshold=171.0\n",
      "150: loss=0.764, reward_mean=111.4,reward_threshold=179.0\n",
      "151: loss=0.704, reward_mean=120.7,reward_threshold=181.7\n",
      "152: loss=0.654, reward_mean=144.4,reward_threshold=272.8\n",
      "153: loss=0.600, reward_mean=147.6,reward_threshold=281.4\n",
      "154: loss=0.631, reward_mean=141.1,reward_threshold=276.0\n",
      "155: loss=0.625, reward_mean=146.0,reward_threshold=273.8\n",
      "156: loss=0.589, reward_mean=152.8,reward_threshold=280.1\n",
      "157: loss=0.592, reward_mean=178.0,reward_threshold=279.8\n",
      "158: loss=0.599, reward_mean=198.7,reward_threshold=291.3\n",
      "159: loss=0.545, reward_mean=210.5,reward_threshold=291.0\n",
      "Environment has been successfullly completed!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 500\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "learning_rate = 0.0025\n",
    "completion_score = 200\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards),np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f,reward_threshold=%.1f\" % (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/gym/wrappers/monitor.py:31: UserWarning: The Monitor wrapper is being deprecated in favor of gym.wrappers.RecordVideo and gym.wrappers.RecordEpisodeStatistics (see https://github.com/openai/gym/issues/2297)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_46353/2918858505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_46353/2351948936.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(env, batch_size, t_max)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0ms_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym.wrappers\n",
    "# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos5\", force=False)\n",
    "generate_batch(env, 1, t_max=500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
