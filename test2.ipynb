{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gym\n",
    "# ! pip3 install box2d-py\n",
    "# ! pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz\n",
    "# ! pip install 'ribs[all]' gym~=0.17.0 Box2D~=2.3.10 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=1000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "            new_s, r, done, info = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    " \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "\n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.384, reward_mean=-171.7,reward_threshold=-96.2\n",
      "1: loss=1.388, reward_mean=-163.6,reward_threshold=-92.9\n",
      "2: loss=1.383, reward_mean=-157.0,reward_threshold=-92.6\n",
      "3: loss=1.379, reward_mean=-168.2,reward_threshold=-83.4\n",
      "4: loss=1.370, reward_mean=-176.8,reward_threshold=-91.2\n",
      "5: loss=1.366, reward_mean=-177.9,reward_threshold=-91.2\n",
      "6: loss=1.369, reward_mean=-159.9,reward_threshold=-75.7\n",
      "7: loss=1.361, reward_mean=-137.1,reward_threshold=-76.3\n",
      "8: loss=1.359, reward_mean=-150.3,reward_threshold=-91.8\n",
      "9: loss=1.356, reward_mean=-124.2,reward_threshold=-74.7\n",
      "10: loss=1.349, reward_mean=-131.6,reward_threshold=-83.6\n",
      "11: loss=1.351, reward_mean=-110.0,reward_threshold=-73.6\n",
      "12: loss=1.352, reward_mean=-119.8,reward_threshold=-68.4\n",
      "13: loss=1.336, reward_mean=-120.0,reward_threshold=-64.6\n",
      "14: loss=1.345, reward_mean=-114.9,reward_threshold=-64.5\n",
      "15: loss=1.340, reward_mean=-117.5,reward_threshold=-65.5\n",
      "16: loss=1.315, reward_mean=-107.8,reward_threshold=-57.1\n",
      "17: loss=1.309, reward_mean=-111.1,reward_threshold=-53.9\n",
      "18: loss=1.293, reward_mean=-85.3,reward_threshold=-44.3\n",
      "19: loss=1.300, reward_mean=-94.2,reward_threshold=-38.8\n",
      "20: loss=1.248, reward_mean=-91.4,reward_threshold=-42.7\n",
      "21: loss=1.254, reward_mean=-81.2,reward_threshold=-29.6\n",
      "22: loss=1.214, reward_mean=-100.8,reward_threshold=-22.8\n",
      "23: loss=1.188, reward_mean=-121.9,reward_threshold=-34.4\n",
      "24: loss=1.154, reward_mean=-125.0,reward_threshold=-28.1\n",
      "25: loss=1.158, reward_mean=-169.5,reward_threshold=-18.8\n",
      "26: loss=1.162, reward_mean=-172.4,reward_threshold=-36.5\n",
      "27: loss=1.168, reward_mean=-147.6,reward_threshold=-19.6\n",
      "28: loss=1.146, reward_mean=-156.2,reward_threshold=-38.6\n",
      "29: loss=1.163, reward_mean=-91.3,reward_threshold=-1.4\n",
      "30: loss=1.188, reward_mean=-53.4,reward_threshold=9.2\n",
      "31: loss=1.180, reward_mean=-49.7,reward_threshold=3.3\n",
      "32: loss=1.192, reward_mean=-48.7,reward_threshold=-3.6\n",
      "33: loss=1.192, reward_mean=-25.2,reward_threshold=9.4\n",
      "34: loss=1.154, reward_mean=-32.3,reward_threshold=-1.9\n",
      "35: loss=1.182, reward_mean=-32.7,reward_threshold=15.3\n",
      "36: loss=1.210, reward_mean=-34.9,reward_threshold=7.5\n",
      "37: loss=1.094, reward_mean=-20.2,reward_threshold=15.7\n",
      "38: loss=1.171, reward_mean=-21.2,reward_threshold=18.0\n",
      "39: loss=1.198, reward_mean=-26.2,reward_threshold=24.9\n",
      "40: loss=1.164, reward_mean=-53.6,reward_threshold=18.6\n",
      "41: loss=1.164, reward_mean=-85.0,reward_threshold=5.3\n",
      "42: loss=1.166, reward_mean=-57.8,reward_threshold=20.7\n",
      "43: loss=1.178, reward_mean=-69.5,reward_threshold=17.7\n",
      "44: loss=1.181, reward_mean=-72.2,reward_threshold=12.9\n",
      "45: loss=1.194, reward_mean=-45.2,reward_threshold=23.0\n",
      "46: loss=1.189, reward_mean=-30.0,reward_threshold=33.7\n",
      "47: loss=1.206, reward_mean=-15.6,reward_threshold=34.4\n",
      "48: loss=1.202, reward_mean=-4.0,reward_threshold=32.7\n",
      "49: loss=1.220, reward_mean=9.7,reward_threshold=38.5\n",
      "50: loss=1.261, reward_mean=11.2,reward_threshold=37.1\n",
      "51: loss=1.253, reward_mean=13.4,reward_threshold=37.7\n",
      "52: loss=1.183, reward_mean=11.4,reward_threshold=32.9\n",
      "53: loss=1.276, reward_mean=16.7,reward_threshold=37.3\n",
      "54: loss=1.260, reward_mean=13.6,reward_threshold=34.4\n",
      "55: loss=1.200, reward_mean=16.3,reward_threshold=36.7\n",
      "56: loss=1.270, reward_mean=18.8,reward_threshold=42.3\n",
      "57: loss=1.298, reward_mean=21.1,reward_threshold=41.4\n",
      "58: loss=1.289, reward_mean=24.9,reward_threshold=47.4\n",
      "59: loss=1.314, reward_mean=34.1,reward_threshold=71.7\n",
      "60: loss=1.301, reward_mean=24.3,reward_threshold=61.9\n",
      "61: loss=1.307, reward_mean=38.6,reward_threshold=67.3\n",
      "62: loss=1.313, reward_mean=39.5,reward_threshold=75.1\n",
      "63: loss=1.307, reward_mean=37.6,reward_threshold=80.6\n",
      "64: loss=1.299, reward_mean=34.1,reward_threshold=72.2\n",
      "65: loss=1.297, reward_mean=31.0,reward_threshold=81.0\n",
      "66: loss=1.299, reward_mean=40.8,reward_threshold=89.6\n",
      "67: loss=1.285, reward_mean=46.1,reward_threshold=92.8\n",
      "68: loss=1.275, reward_mean=51.2,reward_threshold=100.8\n",
      "69: loss=1.259, reward_mean=53.5,reward_threshold=107.6\n",
      "70: loss=1.240, reward_mean=54.5,reward_threshold=111.6\n",
      "71: loss=1.227, reward_mean=59.5,reward_threshold=111.3\n",
      "72: loss=1.204, reward_mean=54.7,reward_threshold=113.8\n",
      "73: loss=1.199, reward_mean=58.4,reward_threshold=118.5\n",
      "74: loss=1.179, reward_mean=50.4,reward_threshold=119.3\n",
      "75: loss=1.172, reward_mean=73.2,reward_threshold=127.4\n",
      "76: loss=1.160, reward_mean=53.2,reward_threshold=124.8\n",
      "77: loss=1.141, reward_mean=56.2,reward_threshold=123.3\n",
      "78: loss=1.155, reward_mean=46.3,reward_threshold=126.1\n",
      "79: loss=1.133, reward_mean=50.0,reward_threshold=125.3\n",
      "80: loss=1.127, reward_mean=67.8,reward_threshold=131.4\n",
      "81: loss=1.105, reward_mean=65.9,reward_threshold=133.5\n",
      "82: loss=1.088, reward_mean=60.2,reward_threshold=128.0\n",
      "83: loss=1.086, reward_mean=59.3,reward_threshold=140.7\n",
      "84: loss=1.095, reward_mean=57.6,reward_threshold=121.9\n",
      "85: loss=1.087, reward_mean=53.3,reward_threshold=131.9\n",
      "86: loss=1.080, reward_mean=72.6,reward_threshold=151.2\n",
      "87: loss=1.058, reward_mean=69.2,reward_threshold=150.9\n",
      "88: loss=1.051, reward_mean=74.2,reward_threshold=152.2\n",
      "89: loss=1.048, reward_mean=70.6,reward_threshold=142.2\n",
      "90: loss=1.036, reward_mean=69.7,reward_threshold=143.7\n",
      "91: loss=1.015, reward_mean=63.0,reward_threshold=143.5\n",
      "92: loss=1.010, reward_mean=66.2,reward_threshold=144.5\n",
      "93: loss=0.998, reward_mean=63.4,reward_threshold=151.9\n",
      "94: loss=0.986, reward_mean=82.7,reward_threshold=160.4\n",
      "95: loss=0.990, reward_mean=65.0,reward_threshold=133.7\n",
      "96: loss=0.977, reward_mean=80.3,reward_threshold=159.8\n",
      "97: loss=0.981, reward_mean=55.6,reward_threshold=151.2\n",
      "98: loss=0.983, reward_mean=84.5,reward_threshold=166.5\n",
      "99: loss=0.972, reward_mean=56.1,reward_threshold=140.7\n",
      "100: loss=0.967, reward_mean=82.2,reward_threshold=157.7\n",
      "101: loss=0.966, reward_mean=82.9,reward_threshold=148.8\n",
      "102: loss=0.949, reward_mean=92.2,reward_threshold=154.1\n",
      "103: loss=0.962, reward_mean=91.4,reward_threshold=164.2\n",
      "104: loss=0.919, reward_mean=85.8,reward_threshold=165.4\n",
      "105: loss=0.886, reward_mean=72.3,reward_threshold=154.6\n",
      "106: loss=0.861, reward_mean=58.7,reward_threshold=158.0\n",
      "107: loss=0.857, reward_mean=111.5,reward_threshold=182.4\n",
      "108: loss=0.804, reward_mean=108.8,reward_threshold=227.8\n",
      "109: loss=0.771, reward_mean=129.2,reward_threshold=243.2\n",
      "110: loss=0.700, reward_mean=148.7,reward_threshold=268.7\n",
      "111: loss=0.689, reward_mean=162.6,reward_threshold=278.9\n",
      "112: loss=0.720, reward_mean=150.2,reward_threshold=271.6\n",
      "113: loss=0.749, reward_mean=144.3,reward_threshold=280.9\n",
      "114: loss=0.723, reward_mean=148.2,reward_threshold=273.3\n",
      "115: loss=0.701, reward_mean=138.7,reward_threshold=260.6\n",
      "116: loss=0.720, reward_mean=121.9,reward_threshold=265.0\n",
      "117: loss=0.715, reward_mean=136.6,reward_threshold=271.3\n",
      "118: loss=0.656, reward_mean=141.0,reward_threshold=281.0\n",
      "119: loss=0.678, reward_mean=87.2,reward_threshold=260.2\n",
      "120: loss=0.632, reward_mean=102.9,reward_threshold=265.9\n",
      "121: loss=0.601, reward_mean=97.8,reward_threshold=265.8\n",
      "122: loss=0.622, reward_mean=95.6,reward_threshold=264.4\n",
      "123: loss=0.582, reward_mean=97.5,reward_threshold=264.1\n",
      "124: loss=0.578, reward_mean=100.9,reward_threshold=265.2\n",
      "125: loss=0.581, reward_mean=94.7,reward_threshold=266.5\n",
      "126: loss=0.624, reward_mean=99.3,reward_threshold=272.3\n",
      "127: loss=0.613, reward_mean=97.2,reward_threshold=273.3\n",
      "128: loss=0.644, reward_mean=109.4,reward_threshold=267.6\n",
      "129: loss=0.598, reward_mean=122.9,reward_threshold=267.6\n",
      "130: loss=0.605, reward_mean=126.8,reward_threshold=275.3\n",
      "131: loss=0.622, reward_mean=120.8,reward_threshold=274.7\n",
      "132: loss=0.609, reward_mean=155.7,reward_threshold=276.2\n",
      "133: loss=0.575, reward_mean=139.4,reward_threshold=277.0\n",
      "134: loss=0.614, reward_mean=125.4,reward_threshold=269.9\n",
      "135: loss=0.572, reward_mean=136.6,reward_threshold=269.9\n",
      "136: loss=0.582, reward_mean=158.0,reward_threshold=276.1\n",
      "137: loss=0.587, reward_mean=137.0,reward_threshold=274.8\n",
      "138: loss=0.597, reward_mean=185.0,reward_threshold=279.7\n",
      "139: loss=0.632, reward_mean=162.1,reward_threshold=278.5\n",
      "140: loss=0.604, reward_mean=184.0,reward_threshold=284.4\n",
      "141: loss=0.653, reward_mean=155.4,reward_threshold=280.5\n",
      "142: loss=0.635, reward_mean=153.6,reward_threshold=279.1\n",
      "143: loss=0.661, reward_mean=206.9,reward_threshold=284.4\n",
      "Environment has been successfullly completed!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 500\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "learning_rate = 0.0025\n",
    "completion_score = 200\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards),np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f,reward_threshold=%.1f\" % (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/gym/wrappers/monitor.py:31: UserWarning: The Monitor wrapper is being deprecated in favor of gym.wrappers.RecordVideo and gym.wrappers.RecordEpisodeStatistics (see https://github.com/openai/gym/issues/2297)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_46353/2918858505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_46353/2351948936.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(env, batch_size, t_max)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0ms_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym.wrappers\n",
    "# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos2\", force=False)\n",
    "generate_batch(env, 1, t_max=500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
