{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gym\n",
    "# ! pip3 install box2d-py\n",
    "# ! pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz\n",
    "# ! pip install 'ribs[all]' gym~=0.17.0 Box2D~=2.3.10 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 128)\n",
    "        self.fc3 = nn.Linear(128, 256)\n",
    "        self.fc4 = nn.Linear(256, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=1000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "            new_s, r, done, info = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    " \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "\n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.385, reward_mean=-175.3,reward_threshold=-87.5\n",
      "1: loss=1.385, reward_mean=-192.3,reward_threshold=-99.9\n",
      "2: loss=1.381, reward_mean=-181.4,reward_threshold=-93.4\n",
      "3: loss=1.384, reward_mean=-171.6,reward_threshold=-92.2\n",
      "4: loss=1.384, reward_mean=-149.3,reward_threshold=-91.3\n",
      "5: loss=1.382, reward_mean=-152.8,reward_threshold=-82.6\n",
      "6: loss=1.379, reward_mean=-143.0,reward_threshold=-87.2\n",
      "7: loss=1.378, reward_mean=-146.5,reward_threshold=-87.0\n",
      "8: loss=1.372, reward_mean=-159.0,reward_threshold=-86.9\n",
      "9: loss=1.364, reward_mean=-136.0,reward_threshold=-78.4\n",
      "10: loss=1.348, reward_mean=-142.6,reward_threshold=-69.3\n",
      "11: loss=1.341, reward_mean=-151.5,reward_threshold=-68.1\n",
      "12: loss=1.327, reward_mean=-155.3,reward_threshold=-79.0\n",
      "13: loss=1.343, reward_mean=-145.4,reward_threshold=-62.2\n",
      "14: loss=1.336, reward_mean=-138.7,reward_threshold=-74.6\n",
      "15: loss=1.315, reward_mean=-130.3,reward_threshold=-57.1\n",
      "16: loss=1.303, reward_mean=-120.3,reward_threshold=-56.0\n",
      "17: loss=1.301, reward_mean=-99.6,reward_threshold=-42.4\n",
      "18: loss=1.284, reward_mean=-103.8,reward_threshold=-53.5\n",
      "19: loss=1.264, reward_mean=-91.8,reward_threshold=-38.6\n",
      "20: loss=1.276, reward_mean=-102.0,reward_threshold=-34.9\n",
      "21: loss=1.240, reward_mean=-101.9,reward_threshold=-26.2\n",
      "22: loss=1.220, reward_mean=-71.6,reward_threshold=-6.6\n",
      "23: loss=1.217, reward_mean=-79.7,reward_threshold=-5.8\n",
      "24: loss=1.205, reward_mean=-70.5,reward_threshold=8.8\n",
      "25: loss=1.151, reward_mean=-79.2,reward_threshold=12.0\n",
      "26: loss=1.210, reward_mean=-49.8,reward_threshold=20.9\n",
      "27: loss=1.200, reward_mean=-17.3,reward_threshold=20.9\n",
      "28: loss=1.176, reward_mean=-12.3,reward_threshold=16.1\n",
      "29: loss=1.167, reward_mean=-3.3,reward_threshold=23.5\n",
      "30: loss=1.176, reward_mean=3.5,reward_threshold=30.1\n",
      "31: loss=1.156, reward_mean=-9.4,reward_threshold=26.6\n",
      "32: loss=1.113, reward_mean=-7.9,reward_threshold=31.2\n",
      "33: loss=1.122, reward_mean=-23.0,reward_threshold=36.2\n",
      "34: loss=1.094, reward_mean=-66.9,reward_threshold=16.5\n",
      "35: loss=1.072, reward_mean=-71.4,reward_threshold=18.4\n",
      "36: loss=1.009, reward_mean=-37.3,reward_threshold=32.6\n",
      "37: loss=0.957, reward_mean=-9.4,reward_threshold=45.9\n",
      "38: loss=0.909, reward_mean=-11.4,reward_threshold=44.0\n",
      "39: loss=0.820, reward_mean=4.8,reward_threshold=49.6\n",
      "40: loss=0.742, reward_mean=8.4,reward_threshold=57.5\n",
      "41: loss=0.695, reward_mean=8.1,reward_threshold=38.7\n",
      "42: loss=0.650, reward_mean=0.7,reward_threshold=45.5\n",
      "43: loss=0.614, reward_mean=-0.9,reward_threshold=57.4\n",
      "44: loss=0.628, reward_mean=9.1,reward_threshold=94.4\n",
      "45: loss=0.610, reward_mean=-33.3,reward_threshold=25.9\n",
      "46: loss=0.557, reward_mean=-41.3,reward_threshold=13.8\n",
      "47: loss=0.562, reward_mean=-21.1,reward_threshold=26.4\n",
      "48: loss=0.609, reward_mean=-26.0,reward_threshold=11.4\n",
      "49: loss=0.717, reward_mean=-25.1,reward_threshold=9.6\n",
      "50: loss=0.733, reward_mean=-5.5,reward_threshold=28.0\n",
      "51: loss=0.735, reward_mean=10.0,reward_threshold=37.5\n",
      "52: loss=0.617, reward_mean=8.5,reward_threshold=30.8\n",
      "53: loss=0.605, reward_mean=14.0,reward_threshold=45.9\n",
      "54: loss=0.660, reward_mean=16.4,reward_threshold=46.3\n",
      "55: loss=0.629, reward_mean=-7.8,reward_threshold=36.2\n",
      "56: loss=0.666, reward_mean=-26.4,reward_threshold=21.3\n",
      "57: loss=0.635, reward_mean=-46.0,reward_threshold=7.7\n",
      "58: loss=0.652, reward_mean=-71.4,reward_threshold=-3.0\n",
      "59: loss=0.617, reward_mean=-72.3,reward_threshold=-9.4\n",
      "60: loss=0.594, reward_mean=-91.0,reward_threshold=-34.0\n",
      "61: loss=0.613, reward_mean=-90.7,reward_threshold=-36.6\n",
      "62: loss=0.613, reward_mean=-76.9,reward_threshold=-22.1\n",
      "63: loss=0.627, reward_mean=-52.9,reward_threshold=-4.8\n",
      "64: loss=0.574, reward_mean=-49.0,reward_threshold=7.5\n",
      "65: loss=0.620, reward_mean=-45.5,reward_threshold=6.2\n",
      "66: loss=0.578, reward_mean=-15.2,reward_threshold=34.5\n",
      "67: loss=0.464, reward_mean=-12.1,reward_threshold=40.1\n",
      "68: loss=0.619, reward_mean=-14.5,reward_threshold=61.3\n",
      "69: loss=0.614, reward_mean=-5.7,reward_threshold=153.8\n",
      "70: loss=0.631, reward_mean=-29.1,reward_threshold=134.5\n",
      "71: loss=0.739, reward_mean=-62.2,reward_threshold=58.5\n",
      "72: loss=0.699, reward_mean=-13.6,reward_threshold=164.1\n",
      "73: loss=0.712, reward_mean=14.1,reward_threshold=166.3\n",
      "74: loss=0.727, reward_mean=17.3,reward_threshold=160.0\n",
      "75: loss=0.675, reward_mean=-1.0,reward_threshold=161.8\n",
      "76: loss=0.661, reward_mean=-2.7,reward_threshold=156.8\n",
      "77: loss=0.591, reward_mean=26.9,reward_threshold=185.8\n",
      "78: loss=0.567, reward_mean=2.8,reward_threshold=172.2\n",
      "79: loss=0.565, reward_mean=49.0,reward_threshold=200.8\n",
      "80: loss=0.496, reward_mean=40.4,reward_threshold=188.5\n",
      "81: loss=0.513, reward_mean=32.4,reward_threshold=170.6\n",
      "82: loss=0.505, reward_mean=36.0,reward_threshold=176.3\n",
      "83: loss=0.486, reward_mean=31.3,reward_threshold=154.6\n",
      "84: loss=0.508, reward_mean=44.6,reward_threshold=166.8\n",
      "85: loss=0.506, reward_mean=18.2,reward_threshold=70.0\n",
      "86: loss=0.476, reward_mean=23.3,reward_threshold=147.3\n",
      "87: loss=0.521, reward_mean=31.2,reward_threshold=178.5\n",
      "88: loss=0.479, reward_mean=22.0,reward_threshold=93.8\n",
      "89: loss=0.497, reward_mean=42.3,reward_threshold=183.3\n",
      "90: loss=0.530, reward_mean=36.5,reward_threshold=171.6\n",
      "91: loss=0.555, reward_mean=55.0,reward_threshold=200.7\n",
      "92: loss=0.586, reward_mean=64.9,reward_threshold=197.5\n",
      "93: loss=0.634, reward_mean=73.2,reward_threshold=189.7\n",
      "94: loss=0.601, reward_mean=65.1,reward_threshold=185.1\n",
      "95: loss=0.588, reward_mean=58.7,reward_threshold=179.8\n",
      "96: loss=0.676, reward_mean=28.5,reward_threshold=167.1\n",
      "97: loss=0.620, reward_mean=11.0,reward_threshold=160.1\n",
      "98: loss=0.653, reward_mean=7.0,reward_threshold=114.4\n",
      "99: loss=0.625, reward_mean=15.1,reward_threshold=117.9\n",
      "100: loss=0.568, reward_mean=40.4,reward_threshold=152.6\n",
      "101: loss=0.614, reward_mean=31.3,reward_threshold=156.5\n",
      "102: loss=0.663, reward_mean=62.9,reward_threshold=168.6\n",
      "103: loss=0.576, reward_mean=32.7,reward_threshold=178.9\n",
      "104: loss=0.617, reward_mean=47.4,reward_threshold=173.1\n",
      "105: loss=0.637, reward_mean=63.5,reward_threshold=182.2\n",
      "106: loss=0.607, reward_mean=66.7,reward_threshold=182.3\n",
      "107: loss=0.658, reward_mean=67.7,reward_threshold=186.5\n",
      "108: loss=0.591, reward_mean=62.1,reward_threshold=191.6\n",
      "109: loss=0.581, reward_mean=50.1,reward_threshold=189.7\n",
      "110: loss=0.585, reward_mean=49.3,reward_threshold=186.8\n",
      "111: loss=0.523, reward_mean=40.1,reward_threshold=174.9\n",
      "112: loss=0.509, reward_mean=21.0,reward_threshold=155.9\n",
      "113: loss=0.478, reward_mean=58.8,reward_threshold=190.8\n",
      "114: loss=0.494, reward_mean=47.3,reward_threshold=196.0\n",
      "115: loss=0.453, reward_mean=55.1,reward_threshold=195.3\n",
      "116: loss=0.486, reward_mean=41.7,reward_threshold=186.1\n",
      "117: loss=0.508, reward_mean=44.1,reward_threshold=190.4\n",
      "118: loss=0.506, reward_mean=50.4,reward_threshold=193.6\n",
      "119: loss=0.466, reward_mean=42.7,reward_threshold=182.8\n",
      "120: loss=0.448, reward_mean=23.5,reward_threshold=173.4\n",
      "121: loss=0.477, reward_mean=44.2,reward_threshold=199.2\n",
      "122: loss=0.464, reward_mean=40.8,reward_threshold=182.3\n",
      "123: loss=0.508, reward_mean=35.0,reward_threshold=187.1\n",
      "124: loss=0.517, reward_mean=74.9,reward_threshold=203.0\n",
      "125: loss=0.470, reward_mean=25.3,reward_threshold=173.4\n",
      "126: loss=0.378, reward_mean=54.9,reward_threshold=195.0\n",
      "127: loss=0.464, reward_mean=53.7,reward_threshold=201.3\n",
      "128: loss=0.470, reward_mean=79.2,reward_threshold=208.7\n",
      "129: loss=0.544, reward_mean=81.0,reward_threshold=198.3\n",
      "130: loss=0.505, reward_mean=70.2,reward_threshold=200.3\n",
      "131: loss=0.524, reward_mean=58.5,reward_threshold=176.8\n",
      "132: loss=0.520, reward_mean=70.1,reward_threshold=189.3\n",
      "133: loss=0.566, reward_mean=106.9,reward_threshold=202.6\n",
      "134: loss=0.593, reward_mean=88.3,reward_threshold=210.2\n",
      "135: loss=0.607, reward_mean=95.9,reward_threshold=220.0\n",
      "136: loss=0.626, reward_mean=94.1,reward_threshold=211.0\n",
      "137: loss=0.623, reward_mean=95.0,reward_threshold=211.5\n",
      "138: loss=0.558, reward_mean=106.6,reward_threshold=215.5\n",
      "139: loss=0.558, reward_mean=115.9,reward_threshold=220.7\n",
      "140: loss=0.629, reward_mean=102.1,reward_threshold=224.6\n",
      "141: loss=0.629, reward_mean=68.8,reward_threshold=198.5\n",
      "142: loss=0.637, reward_mean=80.9,reward_threshold=201.9\n",
      "143: loss=0.569, reward_mean=116.7,reward_threshold=219.2\n",
      "144: loss=0.636, reward_mean=86.9,reward_threshold=224.0\n",
      "145: loss=0.581, reward_mean=105.9,reward_threshold=224.4\n",
      "146: loss=0.656, reward_mean=124.2,reward_threshold=221.3\n",
      "147: loss=0.596, reward_mean=85.1,reward_threshold=218.4\n",
      "148: loss=0.632, reward_mean=121.0,reward_threshold=221.8\n",
      "149: loss=0.590, reward_mean=107.4,reward_threshold=226.5\n",
      "150: loss=0.614, reward_mean=98.3,reward_threshold=219.6\n",
      "151: loss=0.595, reward_mean=91.8,reward_threshold=220.2\n",
      "152: loss=0.580, reward_mean=118.3,reward_threshold=234.0\n",
      "153: loss=0.602, reward_mean=130.4,reward_threshold=236.0\n",
      "154: loss=0.623, reward_mean=115.7,reward_threshold=235.0\n",
      "155: loss=0.627, reward_mean=157.8,reward_threshold=244.5\n",
      "156: loss=0.652, reward_mean=123.4,reward_threshold=227.3\n",
      "157: loss=0.842, reward_mean=105.9,reward_threshold=143.8\n",
      "158: loss=0.624, reward_mean=99.0,reward_threshold=197.0\n",
      "159: loss=0.531, reward_mean=87.2,reward_threshold=213.7\n",
      "160: loss=0.435, reward_mean=78.2,reward_threshold=202.4\n",
      "161: loss=0.429, reward_mean=71.9,reward_threshold=214.6\n",
      "162: loss=0.367, reward_mean=78.7,reward_threshold=233.7\n",
      "163: loss=0.443, reward_mean=42.6,reward_threshold=192.3\n",
      "164: loss=0.410, reward_mean=56.6,reward_threshold=232.9\n",
      "165: loss=0.443, reward_mean=32.7,reward_threshold=206.2\n",
      "166: loss=0.411, reward_mean=59.2,reward_threshold=230.2\n",
      "167: loss=0.405, reward_mean=62.9,reward_threshold=230.6\n",
      "168: loss=0.481, reward_mean=66.2,reward_threshold=227.9\n",
      "169: loss=0.421, reward_mean=92.4,reward_threshold=240.3\n",
      "170: loss=0.533, reward_mean=77.9,reward_threshold=233.0\n",
      "171: loss=0.496, reward_mean=113.0,reward_threshold=241.8\n",
      "172: loss=0.527, reward_mean=108.7,reward_threshold=230.8\n",
      "173: loss=0.523, reward_mean=88.5,reward_threshold=225.8\n",
      "174: loss=0.489, reward_mean=92.3,reward_threshold=219.2\n",
      "175: loss=0.510, reward_mean=94.4,reward_threshold=208.2\n",
      "176: loss=0.576, reward_mean=80.4,reward_threshold=198.9\n",
      "177: loss=0.504, reward_mean=69.5,reward_threshold=197.5\n",
      "178: loss=0.561, reward_mean=64.9,reward_threshold=190.3\n",
      "179: loss=0.572, reward_mean=69.7,reward_threshold=191.5\n",
      "180: loss=0.511, reward_mean=72.3,reward_threshold=195.2\n",
      "181: loss=0.561, reward_mean=103.7,reward_threshold=192.9\n",
      "182: loss=0.596, reward_mean=105.0,reward_threshold=200.1\n",
      "183: loss=0.562, reward_mean=104.4,reward_threshold=204.5\n",
      "184: loss=0.553, reward_mean=109.6,reward_threshold=201.2\n",
      "185: loss=0.563, reward_mean=98.9,reward_threshold=207.9\n",
      "186: loss=0.546, reward_mean=111.0,reward_threshold=220.6\n",
      "187: loss=0.553, reward_mean=89.4,reward_threshold=209.0\n",
      "188: loss=0.574, reward_mean=89.9,reward_threshold=210.6\n",
      "189: loss=0.605, reward_mean=63.1,reward_threshold=192.3\n",
      "190: loss=0.627, reward_mean=65.1,reward_threshold=187.5\n",
      "191: loss=0.649, reward_mean=78.6,reward_threshold=194.2\n",
      "192: loss=0.669, reward_mean=83.3,reward_threshold=220.4\n",
      "193: loss=0.556, reward_mean=133.3,reward_threshold=234.5\n",
      "194: loss=0.556, reward_mean=99.0,reward_threshold=223.8\n",
      "195: loss=0.602, reward_mean=114.0,reward_threshold=228.9\n",
      "196: loss=0.544, reward_mean=111.9,reward_threshold=231.9\n",
      "197: loss=0.542, reward_mean=132.5,reward_threshold=229.4\n",
      "198: loss=0.499, reward_mean=87.6,reward_threshold=221.0\n",
      "199: loss=0.516, reward_mean=87.9,reward_threshold=210.0\n",
      "200: loss=0.462, reward_mean=57.0,reward_threshold=70.1\n",
      "201: loss=0.490, reward_mean=33.2,reward_threshold=47.2\n",
      "202: loss=0.502, reward_mean=29.1,reward_threshold=40.4\n",
      "203: loss=0.521, reward_mean=26.7,reward_threshold=41.4\n",
      "204: loss=0.562, reward_mean=16.7,reward_threshold=40.9\n",
      "205: loss=0.485, reward_mean=18.3,reward_threshold=36.9\n",
      "206: loss=0.566, reward_mean=8.5,reward_threshold=27.1\n",
      "207: loss=0.460, reward_mean=23.2,reward_threshold=31.9\n",
      "208: loss=0.439, reward_mean=16.5,reward_threshold=32.5\n",
      "209: loss=0.511, reward_mean=12.0,reward_threshold=21.8\n",
      "210: loss=0.472, reward_mean=21.2,reward_threshold=38.9\n",
      "211: loss=0.444, reward_mean=43.9,reward_threshold=53.0\n",
      "212: loss=0.387, reward_mean=57.7,reward_threshold=158.6\n",
      "213: loss=0.466, reward_mean=64.9,reward_threshold=166.3\n",
      "214: loss=0.509, reward_mean=78.0,reward_threshold=205.1\n",
      "215: loss=0.480, reward_mean=85.0,reward_threshold=211.2\n",
      "216: loss=0.509, reward_mean=91.7,reward_threshold=216.3\n",
      "217: loss=0.511, reward_mean=98.4,reward_threshold=224.2\n",
      "218: loss=0.515, reward_mean=81.1,reward_threshold=224.2\n",
      "219: loss=0.558, reward_mean=103.6,reward_threshold=227.1\n",
      "220: loss=0.535, reward_mean=69.4,reward_threshold=208.1\n",
      "221: loss=0.585, reward_mean=79.8,reward_threshold=211.3\n",
      "222: loss=0.574, reward_mean=29.3,reward_threshold=187.3\n",
      "223: loss=0.585, reward_mean=13.2,reward_threshold=178.5\n",
      "224: loss=0.611, reward_mean=22.1,reward_threshold=172.4\n",
      "225: loss=0.653, reward_mean=28.3,reward_threshold=174.5\n",
      "226: loss=0.589, reward_mean=-1.5,reward_threshold=172.7\n",
      "227: loss=0.576, reward_mean=13.2,reward_threshold=167.5\n",
      "228: loss=0.571, reward_mean=-36.9,reward_threshold=171.1\n",
      "229: loss=0.496, reward_mean=58.6,reward_threshold=189.7\n",
      "230: loss=0.472, reward_mean=54.4,reward_threshold=196.2\n",
      "231: loss=0.458, reward_mean=58.0,reward_threshold=212.3\n",
      "232: loss=0.436, reward_mean=39.4,reward_threshold=194.4\n",
      "233: loss=0.434, reward_mean=46.2,reward_threshold=202.0\n",
      "234: loss=0.437, reward_mean=11.4,reward_threshold=191.0\n",
      "235: loss=0.457, reward_mean=-14.1,reward_threshold=174.2\n",
      "236: loss=0.397, reward_mean=-30.0,reward_threshold=183.8\n",
      "237: loss=0.500, reward_mean=-45.2,reward_threshold=164.3\n",
      "238: loss=0.435, reward_mean=-85.4,reward_threshold=118.4\n",
      "239: loss=0.485, reward_mean=-119.3,reward_threshold=-61.0\n",
      "240: loss=0.482, reward_mean=-145.6,reward_threshold=-69.4\n",
      "241: loss=0.598, reward_mean=-130.0,reward_threshold=-76.0\n",
      "242: loss=0.444, reward_mean=-97.5,reward_threshold=-45.8\n",
      "243: loss=0.430, reward_mean=-61.6,reward_threshold=-46.7\n",
      "244: loss=0.453, reward_mean=-58.6,reward_threshold=-41.9\n",
      "245: loss=0.450, reward_mean=-40.0,reward_threshold=-25.0\n",
      "246: loss=0.404, reward_mean=-26.1,reward_threshold=-8.1\n",
      "247: loss=0.443, reward_mean=-19.5,reward_threshold=-3.4\n",
      "248: loss=0.460, reward_mean=-6.6,reward_threshold=1.1\n",
      "249: loss=0.332, reward_mean=-6.2,reward_threshold=11.2\n",
      "250: loss=0.391, reward_mean=5.6,reward_threshold=25.3\n",
      "251: loss=0.415, reward_mean=8.9,reward_threshold=40.3\n",
      "252: loss=0.406, reward_mean=13.8,reward_threshold=50.7\n",
      "253: loss=0.390, reward_mean=13.4,reward_threshold=44.6\n",
      "254: loss=0.334, reward_mean=32.7,reward_threshold=189.0\n",
      "255: loss=0.405, reward_mean=64.6,reward_threshold=201.2\n",
      "256: loss=0.384, reward_mean=67.0,reward_threshold=212.9\n",
      "257: loss=0.403, reward_mean=80.0,reward_threshold=218.3\n",
      "258: loss=0.370, reward_mean=86.4,reward_threshold=216.6\n",
      "259: loss=0.405, reward_mean=99.4,reward_threshold=217.9\n",
      "260: loss=0.433, reward_mean=114.3,reward_threshold=220.8\n",
      "261: loss=0.438, reward_mean=100.2,reward_threshold=231.6\n",
      "262: loss=0.458, reward_mean=107.8,reward_threshold=225.1\n",
      "263: loss=0.500, reward_mean=106.6,reward_threshold=221.3\n",
      "264: loss=0.510, reward_mean=84.5,reward_threshold=217.0\n",
      "265: loss=0.536, reward_mean=76.4,reward_threshold=210.1\n",
      "266: loss=0.522, reward_mean=93.4,reward_threshold=214.3\n",
      "267: loss=0.508, reward_mean=73.7,reward_threshold=215.3\n",
      "268: loss=0.547, reward_mean=90.0,reward_threshold=214.4\n",
      "269: loss=0.496, reward_mean=95.5,reward_threshold=211.1\n",
      "270: loss=0.556, reward_mean=88.2,reward_threshold=216.4\n",
      "271: loss=0.555, reward_mean=111.1,reward_threshold=214.0\n",
      "272: loss=0.583, reward_mean=82.5,reward_threshold=209.2\n",
      "273: loss=0.585, reward_mean=114.5,reward_threshold=206.8\n",
      "274: loss=0.599, reward_mean=70.5,reward_threshold=203.0\n",
      "275: loss=0.601, reward_mean=53.0,reward_threshold=195.9\n",
      "276: loss=0.601, reward_mean=67.2,reward_threshold=203.1\n",
      "277: loss=0.576, reward_mean=94.1,reward_threshold=205.4\n",
      "278: loss=0.598, reward_mean=91.8,reward_threshold=209.2\n",
      "279: loss=0.575, reward_mean=94.0,reward_threshold=215.7\n",
      "280: loss=0.537, reward_mean=110.2,reward_threshold=215.8\n",
      "281: loss=0.520, reward_mean=121.8,reward_threshold=225.9\n",
      "282: loss=0.516, reward_mean=120.9,reward_threshold=229.6\n",
      "283: loss=0.455, reward_mean=120.3,reward_threshold=215.5\n",
      "284: loss=0.438, reward_mean=137.8,reward_threshold=230.8\n",
      "285: loss=0.445, reward_mean=114.7,reward_threshold=223.1\n",
      "286: loss=0.418, reward_mean=147.5,reward_threshold=241.3\n",
      "287: loss=0.380, reward_mean=142.1,reward_threshold=237.0\n",
      "288: loss=0.391, reward_mean=133.9,reward_threshold=240.3\n",
      "289: loss=0.391, reward_mean=132.9,reward_threshold=238.0\n",
      "290: loss=0.373, reward_mean=121.6,reward_threshold=233.1\n",
      "291: loss=0.361, reward_mean=98.9,reward_threshold=223.0\n",
      "292: loss=0.393, reward_mean=114.7,reward_threshold=225.3\n",
      "293: loss=0.368, reward_mean=125.5,reward_threshold=235.8\n",
      "294: loss=0.367, reward_mean=86.1,reward_threshold=230.1\n",
      "295: loss=0.389, reward_mean=84.3,reward_threshold=219.3\n",
      "296: loss=0.391, reward_mean=104.3,reward_threshold=233.2\n",
      "297: loss=0.336, reward_mean=92.0,reward_threshold=208.2\n",
      "298: loss=0.380, reward_mean=122.9,reward_threshold=238.4\n",
      "299: loss=0.387, reward_mean=101.7,reward_threshold=228.4\n",
      "300: loss=0.377, reward_mean=122.3,reward_threshold=245.3\n",
      "301: loss=0.395, reward_mean=162.4,reward_threshold=250.1\n",
      "302: loss=0.343, reward_mean=142.6,reward_threshold=236.6\n",
      "303: loss=0.379, reward_mean=141.1,reward_threshold=241.3\n",
      "304: loss=0.368, reward_mean=147.0,reward_threshold=247.8\n",
      "305: loss=0.387, reward_mean=145.2,reward_threshold=246.7\n",
      "306: loss=0.369, reward_mean=139.4,reward_threshold=228.1\n",
      "307: loss=0.368, reward_mean=140.7,reward_threshold=232.0\n",
      "308: loss=0.401, reward_mean=151.5,reward_threshold=244.0\n",
      "309: loss=0.380, reward_mean=177.4,reward_threshold=255.1\n",
      "310: loss=0.394, reward_mean=162.4,reward_threshold=251.4\n",
      "311: loss=0.376, reward_mean=165.4,reward_threshold=243.9\n",
      "312: loss=0.376, reward_mean=158.9,reward_threshold=249.9\n",
      "313: loss=0.419, reward_mean=151.9,reward_threshold=239.1\n",
      "314: loss=0.388, reward_mean=160.5,reward_threshold=247.8\n",
      "315: loss=0.420, reward_mean=177.4,reward_threshold=257.7\n",
      "316: loss=0.428, reward_mean=153.5,reward_threshold=243.0\n",
      "317: loss=0.420, reward_mean=177.7,reward_threshold=252.2\n",
      "318: loss=0.399, reward_mean=156.0,reward_threshold=247.3\n",
      "319: loss=0.445, reward_mean=153.1,reward_threshold=238.9\n",
      "320: loss=0.466, reward_mean=155.2,reward_threshold=243.2\n",
      "321: loss=0.431, reward_mean=172.9,reward_threshold=255.0\n",
      "322: loss=0.474, reward_mean=163.6,reward_threshold=250.3\n",
      "323: loss=0.399, reward_mean=186.5,reward_threshold=257.7\n",
      "324: loss=0.453, reward_mean=164.7,reward_threshold=251.1\n",
      "325: loss=0.448, reward_mean=180.1,reward_threshold=248.6\n",
      "326: loss=0.445, reward_mean=148.8,reward_threshold=238.2\n",
      "327: loss=0.446, reward_mean=171.4,reward_threshold=247.6\n",
      "328: loss=0.467, reward_mean=158.9,reward_threshold=243.1\n",
      "329: loss=0.484, reward_mean=156.9,reward_threshold=239.2\n",
      "330: loss=0.462, reward_mean=161.6,reward_threshold=245.2\n",
      "331: loss=0.460, reward_mean=161.7,reward_threshold=240.8\n",
      "332: loss=0.454, reward_mean=139.8,reward_threshold=230.7\n",
      "333: loss=0.480, reward_mean=159.7,reward_threshold=244.9\n",
      "334: loss=0.478, reward_mean=147.4,reward_threshold=247.8\n",
      "335: loss=0.464, reward_mean=151.4,reward_threshold=245.7\n",
      "336: loss=0.436, reward_mean=135.5,reward_threshold=249.5\n",
      "337: loss=0.456, reward_mean=146.5,reward_threshold=252.7\n",
      "338: loss=0.489, reward_mean=115.1,reward_threshold=244.9\n",
      "339: loss=0.475, reward_mean=108.4,reward_threshold=252.5\n",
      "340: loss=0.467, reward_mean=97.1,reward_threshold=229.0\n",
      "341: loss=0.449, reward_mean=80.8,reward_threshold=235.4\n",
      "342: loss=0.487, reward_mean=84.2,reward_threshold=223.8\n",
      "343: loss=0.552, reward_mean=74.3,reward_threshold=134.4\n",
      "344: loss=0.671, reward_mean=68.9,reward_threshold=137.8\n",
      "345: loss=0.757, reward_mean=58.5,reward_threshold=132.2\n",
      "346: loss=0.629, reward_mean=62.4,reward_threshold=141.0\n",
      "347: loss=0.562, reward_mean=50.5,reward_threshold=121.7\n",
      "348: loss=0.642, reward_mean=60.8,reward_threshold=133.8\n",
      "349: loss=0.703, reward_mean=66.8,reward_threshold=130.3\n",
      "350: loss=0.461, reward_mean=78.5,reward_threshold=176.0\n",
      "351: loss=0.624, reward_mean=61.1,reward_threshold=138.0\n",
      "352: loss=0.505, reward_mean=86.7,reward_threshold=144.6\n",
      "353: loss=0.447, reward_mean=106.4,reward_threshold=212.4\n",
      "354: loss=0.442, reward_mean=133.9,reward_threshold=244.4\n",
      "355: loss=0.386, reward_mean=140.3,reward_threshold=240.8\n",
      "356: loss=0.372, reward_mean=140.5,reward_threshold=238.7\n",
      "357: loss=0.440, reward_mean=161.1,reward_threshold=245.5\n",
      "358: loss=0.414, reward_mean=123.5,reward_threshold=243.9\n",
      "359: loss=0.427, reward_mean=119.8,reward_threshold=227.3\n",
      "360: loss=0.452, reward_mean=129.4,reward_threshold=236.5\n",
      "361: loss=0.419, reward_mean=112.6,reward_threshold=228.2\n",
      "362: loss=0.436, reward_mean=127.8,reward_threshold=238.5\n",
      "363: loss=0.374, reward_mean=129.1,reward_threshold=237.6\n",
      "364: loss=0.413, reward_mean=121.4,reward_threshold=240.3\n",
      "365: loss=0.364, reward_mean=140.3,reward_threshold=236.4\n",
      "366: loss=0.392, reward_mean=123.3,reward_threshold=238.4\n",
      "367: loss=0.380, reward_mean=111.5,reward_threshold=236.9\n",
      "368: loss=0.418, reward_mean=121.0,reward_threshold=242.3\n",
      "369: loss=0.408, reward_mean=114.3,reward_threshold=226.9\n",
      "370: loss=0.370, reward_mean=126.2,reward_threshold=231.1\n",
      "371: loss=0.429, reward_mean=137.8,reward_threshold=243.9\n",
      "372: loss=0.484, reward_mean=144.7,reward_threshold=244.8\n",
      "373: loss=0.470, reward_mean=118.6,reward_threshold=230.2\n",
      "374: loss=0.549, reward_mean=111.1,reward_threshold=225.6\n",
      "375: loss=0.582, reward_mean=109.0,reward_threshold=222.3\n",
      "376: loss=0.634, reward_mean=102.5,reward_threshold=228.3\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 500\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "learning_rate = 0.0025\n",
    "completion_score = 200\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards),np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f,reward_threshold=%.1f\" % (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/gym/wrappers/monitor.py:31: UserWarning: The Monitor wrapper is being deprecated in favor of gym.wrappers.RecordVideo and gym.wrappers.RecordEpisodeStatistics (see https://github.com/openai/gym/issues/2297)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_46353/2918858505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"videos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_46353/2351948936.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(env, batch_size, t_max)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0ms_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym.wrappers\n",
    "# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos4\", force=False)\n",
    "generate_batch(env, 1, t_max=500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
