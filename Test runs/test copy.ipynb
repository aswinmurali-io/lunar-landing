{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gym\n",
    "# ! pip3 install box2d-py\n",
    "# ! pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz\n",
    "# ! pip install 'ribs[all]' gym~=0.17.0 Box2D~=2.3.10 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "session_size = 500\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "learning_rate = 0.0025\n",
    "completion_score = 200\n",
    "\n",
    "# BATCH_SIZE = 100\n",
    "# SESSION_SIZE = 500\n",
    "# PERCENTILE = 80\n",
    "# LEARNING_RATE = 0.0025\n",
    "# COMPLETION_SCORE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=1000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            print(s_v)\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "            new_s, r, done, info = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    " \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "\n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of States - 8\n",
      "No of Actions - 4\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(f\"No of States - {n_states}\")\n",
    "print(f\"No of Actions - {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.384, reward_mean=-167.9,reward_threshold=-82.3\n",
      "1: loss=1.384, reward_mean=-168.3,reward_threshold=-96.8\n",
      "2: loss=1.380, reward_mean=-172.8,reward_threshold=-95.4\n",
      "3: loss=1.376, reward_mean=-169.3,reward_threshold=-92.7\n",
      "4: loss=1.372, reward_mean=-164.8,reward_threshold=-87.4\n",
      "5: loss=1.373, reward_mean=-167.6,reward_threshold=-81.2\n",
      "6: loss=1.376, reward_mean=-161.8,reward_threshold=-82.8\n",
      "7: loss=1.367, reward_mean=-145.2,reward_threshold=-80.1\n",
      "8: loss=1.353, reward_mean=-163.4,reward_threshold=-80.6\n",
      "9: loss=1.365, reward_mean=-167.0,reward_threshold=-76.7\n",
      "10: loss=1.357, reward_mean=-182.1,reward_threshold=-93.0\n",
      "11: loss=1.342, reward_mean=-184.9,reward_threshold=-83.4\n",
      "12: loss=1.345, reward_mean=-186.9,reward_threshold=-75.9\n",
      "13: loss=1.324, reward_mean=-166.8,reward_threshold=-66.9\n",
      "14: loss=1.332, reward_mean=-170.9,reward_threshold=-75.7\n",
      "15: loss=1.324, reward_mean=-163.9,reward_threshold=-62.1\n",
      "16: loss=1.315, reward_mean=-149.6,reward_threshold=-69.1\n",
      "17: loss=1.309, reward_mean=-135.0,reward_threshold=-63.0\n",
      "18: loss=1.319, reward_mean=-150.5,reward_threshold=-61.3\n",
      "19: loss=1.322, reward_mean=-156.1,reward_threshold=-67.7\n",
      "20: loss=1.302, reward_mean=-169.7,reward_threshold=-67.0\n",
      "21: loss=1.296, reward_mean=-170.5,reward_threshold=-52.2\n",
      "22: loss=1.298, reward_mean=-166.2,reward_threshold=-47.1\n",
      "23: loss=1.281, reward_mean=-191.4,reward_threshold=-67.0\n",
      "24: loss=1.267, reward_mean=-221.1,reward_threshold=-80.2\n",
      "25: loss=1.261, reward_mean=-188.6,reward_threshold=-70.6\n",
      "26: loss=1.250, reward_mean=-153.9,reward_threshold=-50.5\n",
      "27: loss=1.262, reward_mean=-164.6,reward_threshold=-54.8\n",
      "28: loss=1.272, reward_mean=-175.9,reward_threshold=-58.6\n",
      "29: loss=1.256, reward_mean=-155.7,reward_threshold=-40.2\n",
      "30: loss=1.249, reward_mean=-143.2,reward_threshold=-50.5\n",
      "31: loss=1.247, reward_mean=-125.1,reward_threshold=-42.3\n",
      "32: loss=1.276, reward_mean=-139.8,reward_threshold=-46.5\n",
      "33: loss=1.232, reward_mean=-133.2,reward_threshold=-50.2\n",
      "34: loss=1.238, reward_mean=-121.3,reward_threshold=-42.1\n",
      "35: loss=1.269, reward_mean=-99.2,reward_threshold=-31.9\n",
      "36: loss=1.251, reward_mean=-93.4,reward_threshold=-28.2\n",
      "37: loss=1.272, reward_mean=-99.8,reward_threshold=-39.1\n",
      "38: loss=1.268, reward_mean=-96.0,reward_threshold=-27.8\n",
      "39: loss=1.255, reward_mean=-97.3,reward_threshold=-36.0\n",
      "40: loss=1.264, reward_mean=-87.2,reward_threshold=-25.4\n",
      "41: loss=1.253, reward_mean=-96.8,reward_threshold=-37.1\n",
      "42: loss=1.268, reward_mean=-95.0,reward_threshold=-25.7\n",
      "43: loss=1.241, reward_mean=-92.1,reward_threshold=-20.0\n",
      "44: loss=1.224, reward_mean=-100.6,reward_threshold=-21.2\n",
      "45: loss=1.235, reward_mean=-110.1,reward_threshold=-21.8\n",
      "46: loss=1.251, reward_mean=-94.2,reward_threshold=-16.3\n",
      "47: loss=1.196, reward_mean=-78.3,reward_threshold=-15.1\n",
      "48: loss=1.184, reward_mean=-94.0,reward_threshold=-7.6\n",
      "49: loss=1.201, reward_mean=-83.7,reward_threshold=-6.7\n",
      "50: loss=1.214, reward_mean=-110.7,reward_threshold=-14.9\n",
      "51: loss=1.204, reward_mean=-103.8,reward_threshold=-3.3\n",
      "52: loss=1.169, reward_mean=-62.9,reward_threshold=0.6\n",
      "53: loss=1.192, reward_mean=-82.7,reward_threshold=-3.7\n",
      "54: loss=1.203, reward_mean=-70.0,reward_threshold=-16.3\n",
      "55: loss=1.201, reward_mean=-60.9,reward_threshold=-0.7\n",
      "56: loss=1.204, reward_mean=-89.0,reward_threshold=-6.0\n",
      "57: loss=1.226, reward_mean=-44.0,reward_threshold=5.7\n",
      "58: loss=1.200, reward_mean=-57.4,reward_threshold=10.5\n",
      "59: loss=1.216, reward_mean=-53.6,reward_threshold=4.0\n",
      "60: loss=1.241, reward_mean=-57.0,reward_threshold=-0.0\n",
      "61: loss=1.214, reward_mean=-56.8,reward_threshold=10.9\n",
      "62: loss=1.221, reward_mean=-47.9,reward_threshold=10.2\n",
      "63: loss=1.199, reward_mean=-48.4,reward_threshold=5.7\n",
      "64: loss=1.199, reward_mean=-45.2,reward_threshold=9.4\n",
      "65: loss=1.227, reward_mean=-55.2,reward_threshold=6.8\n",
      "66: loss=1.171, reward_mean=-46.3,reward_threshold=4.9\n",
      "67: loss=1.235, reward_mean=-46.4,reward_threshold=4.5\n",
      "68: loss=1.211, reward_mean=-53.6,reward_threshold=12.6\n",
      "69: loss=1.225, reward_mean=-39.0,reward_threshold=15.5\n",
      "70: loss=1.223, reward_mean=-36.3,reward_threshold=11.5\n",
      "71: loss=1.233, reward_mean=-36.4,reward_threshold=28.5\n",
      "72: loss=1.252, reward_mean=-35.5,reward_threshold=18.3\n",
      "73: loss=1.244, reward_mean=-28.4,reward_threshold=15.2\n",
      "74: loss=1.236, reward_mean=-30.1,reward_threshold=24.3\n",
      "75: loss=1.231, reward_mean=-25.9,reward_threshold=22.3\n",
      "76: loss=1.239, reward_mean=-18.2,reward_threshold=25.4\n",
      "77: loss=1.227, reward_mean=-21.1,reward_threshold=28.3\n",
      "78: loss=1.248, reward_mean=-14.9,reward_threshold=35.9\n",
      "79: loss=1.217, reward_mean=-29.6,reward_threshold=29.2\n",
      "80: loss=1.226, reward_mean=-15.2,reward_threshold=26.1\n",
      "81: loss=1.214, reward_mean=-15.1,reward_threshold=25.1\n",
      "82: loss=1.220, reward_mean=-18.0,reward_threshold=31.5\n",
      "83: loss=1.202, reward_mean=-14.6,reward_threshold=26.9\n",
      "84: loss=1.221, reward_mean=-19.5,reward_threshold=32.0\n",
      "85: loss=1.222, reward_mean=-24.6,reward_threshold=33.7\n",
      "86: loss=1.220, reward_mean=-11.3,reward_threshold=36.0\n",
      "87: loss=1.228, reward_mean=-23.8,reward_threshold=28.4\n",
      "88: loss=1.231, reward_mean=-29.3,reward_threshold=32.5\n",
      "89: loss=1.218, reward_mean=-49.6,reward_threshold=22.0\n",
      "90: loss=1.212, reward_mean=-31.1,reward_threshold=35.1\n",
      "91: loss=1.231, reward_mean=-28.9,reward_threshold=39.0\n",
      "92: loss=1.216, reward_mean=-56.6,reward_threshold=32.9\n",
      "93: loss=1.216, reward_mean=-62.3,reward_threshold=20.4\n",
      "94: loss=1.238, reward_mean=-37.2,reward_threshold=42.7\n",
      "95: loss=1.208, reward_mean=-54.5,reward_threshold=34.3\n",
      "96: loss=1.212, reward_mean=-72.6,reward_threshold=27.0\n",
      "97: loss=1.210, reward_mean=-49.0,reward_threshold=31.2\n",
      "98: loss=1.213, reward_mean=-55.4,reward_threshold=28.1\n",
      "99: loss=1.190, reward_mean=-54.9,reward_threshold=30.3\n",
      "100: loss=1.207, reward_mean=-67.7,reward_threshold=37.8\n",
      "101: loss=1.215, reward_mean=-48.9,reward_threshold=22.6\n",
      "102: loss=1.218, reward_mean=-48.0,reward_threshold=26.0\n",
      "103: loss=1.215, reward_mean=-25.3,reward_threshold=39.3\n",
      "104: loss=1.218, reward_mean=-11.2,reward_threshold=42.3\n",
      "105: loss=1.207, reward_mean=-13.4,reward_threshold=38.4\n",
      "106: loss=1.236, reward_mean=1.1,reward_threshold=37.4\n",
      "107: loss=1.214, reward_mean=-11.1,reward_threshold=34.3\n",
      "108: loss=1.230, reward_mean=-14.8,reward_threshold=50.9\n",
      "109: loss=1.241, reward_mean=-10.8,reward_threshold=40.2\n",
      "110: loss=1.214, reward_mean=4.3,reward_threshold=37.2\n",
      "111: loss=1.243, reward_mean=-3.7,reward_threshold=41.0\n",
      "112: loss=1.261, reward_mean=-5.5,reward_threshold=39.9\n",
      "113: loss=1.253, reward_mean=-1.9,reward_threshold=33.8\n",
      "114: loss=1.254, reward_mean=11.0,reward_threshold=48.1\n",
      "115: loss=1.254, reward_mean=4.4,reward_threshold=41.9\n",
      "116: loss=1.265, reward_mean=11.4,reward_threshold=45.7\n",
      "117: loss=1.268, reward_mean=2.2,reward_threshold=30.2\n",
      "118: loss=1.253, reward_mean=4.3,reward_threshold=40.0\n",
      "119: loss=1.273, reward_mean=17.9,reward_threshold=51.9\n",
      "120: loss=1.280, reward_mean=12.5,reward_threshold=53.3\n",
      "121: loss=1.279, reward_mean=11.7,reward_threshold=54.4\n",
      "122: loss=1.278, reward_mean=10.7,reward_threshold=52.5\n",
      "123: loss=1.278, reward_mean=5.6,reward_threshold=54.2\n",
      "124: loss=1.262, reward_mean=5.7,reward_threshold=49.4\n",
      "125: loss=1.279, reward_mean=16.8,reward_threshold=60.0\n",
      "126: loss=1.278, reward_mean=8.1,reward_threshold=62.5\n",
      "127: loss=1.261, reward_mean=2.5,reward_threshold=45.8\n",
      "128: loss=1.279, reward_mean=-5.3,reward_threshold=49.3\n",
      "129: loss=1.272, reward_mean=4.0,reward_threshold=54.4\n",
      "130: loss=1.273, reward_mean=23.0,reward_threshold=73.4\n",
      "131: loss=1.270, reward_mean=14.6,reward_threshold=57.2\n",
      "132: loss=1.273, reward_mean=2.0,reward_threshold=55.9\n",
      "133: loss=1.275, reward_mean=17.1,reward_threshold=69.3\n",
      "134: loss=1.267, reward_mean=18.5,reward_threshold=80.3\n",
      "135: loss=1.261, reward_mean=14.8,reward_threshold=69.5\n",
      "136: loss=1.255, reward_mean=0.7,reward_threshold=72.8\n",
      "137: loss=1.261, reward_mean=11.6,reward_threshold=67.4\n",
      "138: loss=1.250, reward_mean=15.0,reward_threshold=67.9\n",
      "139: loss=1.242, reward_mean=13.6,reward_threshold=84.6\n",
      "140: loss=1.234, reward_mean=18.0,reward_threshold=72.5\n",
      "141: loss=1.233, reward_mean=4.9,reward_threshold=86.7\n",
      "142: loss=1.227, reward_mean=24.0,reward_threshold=89.5\n",
      "143: loss=1.219, reward_mean=1.5,reward_threshold=73.9\n",
      "144: loss=1.209, reward_mean=10.4,reward_threshold=74.7\n",
      "145: loss=1.202, reward_mean=9.9,reward_threshold=81.1\n",
      "146: loss=1.200, reward_mean=14.7,reward_threshold=84.2\n",
      "147: loss=1.193, reward_mean=24.6,reward_threshold=101.2\n",
      "148: loss=1.184, reward_mean=7.4,reward_threshold=77.0\n",
      "149: loss=1.187, reward_mean=18.0,reward_threshold=84.8\n",
      "150: loss=1.184, reward_mean=19.5,reward_threshold=102.3\n",
      "151: loss=1.181, reward_mean=2.7,reward_threshold=91.2\n",
      "152: loss=1.160, reward_mean=-0.6,reward_threshold=98.6\n",
      "153: loss=1.152, reward_mean=22.7,reward_threshold=97.6\n",
      "154: loss=1.136, reward_mean=20.5,reward_threshold=107.6\n",
      "155: loss=1.129, reward_mean=24.0,reward_threshold=97.3\n",
      "156: loss=1.101, reward_mean=8.6,reward_threshold=103.0\n",
      "157: loss=1.081, reward_mean=40.2,reward_threshold=129.2\n",
      "158: loss=1.053, reward_mean=10.1,reward_threshold=122.3\n",
      "159: loss=1.048, reward_mean=0.2,reward_threshold=87.0\n",
      "160: loss=1.004, reward_mean=56.4,reward_threshold=133.6\n",
      "161: loss=0.977, reward_mean=41.8,reward_threshold=129.1\n",
      "162: loss=0.965, reward_mean=32.3,reward_threshold=119.2\n",
      "163: loss=0.938, reward_mean=26.0,reward_threshold=116.2\n",
      "164: loss=0.918, reward_mean=3.8,reward_threshold=67.4\n",
      "165: loss=0.889, reward_mean=33.9,reward_threshold=132.9\n",
      "166: loss=0.915, reward_mean=24.7,reward_threshold=72.3\n",
      "167: loss=0.875, reward_mean=23.9,reward_threshold=137.9\n",
      "168: loss=0.852, reward_mean=41.0,reward_threshold=141.8\n",
      "169: loss=0.841, reward_mean=48.9,reward_threshold=135.5\n",
      "170: loss=0.846, reward_mean=43.3,reward_threshold=144.9\n",
      "171: loss=0.816, reward_mean=53.2,reward_threshold=174.6\n",
      "172: loss=0.811, reward_mean=46.7,reward_threshold=197.6\n",
      "173: loss=0.803, reward_mean=69.5,reward_threshold=215.2\n",
      "174: loss=0.825, reward_mean=23.8,reward_threshold=197.1\n",
      "175: loss=0.767, reward_mean=53.7,reward_threshold=246.9\n",
      "176: loss=0.789, reward_mean=51.2,reward_threshold=236.5\n",
      "177: loss=0.786, reward_mean=53.4,reward_threshold=235.4\n",
      "178: loss=0.788, reward_mean=70.3,reward_threshold=248.4\n",
      "179: loss=0.804, reward_mean=68.9,reward_threshold=242.8\n",
      "180: loss=0.779, reward_mean=29.1,reward_threshold=234.1\n",
      "181: loss=0.786, reward_mean=70.1,reward_threshold=245.7\n",
      "182: loss=0.785, reward_mean=58.6,reward_threshold=245.0\n",
      "183: loss=0.815, reward_mean=58.5,reward_threshold=235.8\n",
      "184: loss=0.793, reward_mean=28.6,reward_threshold=241.7\n",
      "185: loss=0.770, reward_mean=51.7,reward_threshold=246.2\n",
      "186: loss=0.787, reward_mean=40.2,reward_threshold=249.6\n",
      "187: loss=0.785, reward_mean=72.4,reward_threshold=247.3\n",
      "188: loss=0.779, reward_mean=36.6,reward_threshold=243.7\n",
      "189: loss=0.766, reward_mean=79.5,reward_threshold=253.0\n",
      "190: loss=0.765, reward_mean=61.3,reward_threshold=252.5\n",
      "191: loss=0.765, reward_mean=52.0,reward_threshold=255.4\n",
      "192: loss=0.757, reward_mean=73.9,reward_threshold=238.3\n",
      "193: loss=0.752, reward_mean=71.2,reward_threshold=247.6\n",
      "194: loss=0.762, reward_mean=103.7,reward_threshold=260.0\n",
      "195: loss=0.772, reward_mean=89.5,reward_threshold=260.9\n",
      "196: loss=0.760, reward_mean=90.3,reward_threshold=266.0\n",
      "197: loss=0.741, reward_mean=84.6,reward_threshold=245.5\n",
      "198: loss=0.759, reward_mean=68.9,reward_threshold=245.8\n",
      "199: loss=0.765, reward_mean=99.1,reward_threshold=269.8\n",
      "200: loss=0.736, reward_mean=78.8,reward_threshold=247.6\n",
      "201: loss=0.721, reward_mean=104.9,reward_threshold=263.2\n",
      "202: loss=0.766, reward_mean=90.8,reward_threshold=258.3\n",
      "203: loss=0.724, reward_mean=101.6,reward_threshold=252.0\n",
      "204: loss=0.766, reward_mean=113.2,reward_threshold=266.0\n",
      "205: loss=0.786, reward_mean=103.4,reward_threshold=257.5\n",
      "206: loss=0.747, reward_mean=119.8,reward_threshold=261.0\n",
      "207: loss=0.733, reward_mean=139.2,reward_threshold=271.0\n",
      "208: loss=0.766, reward_mean=132.5,reward_threshold=265.9\n",
      "209: loss=0.736, reward_mean=131.7,reward_threshold=269.8\n",
      "210: loss=0.759, reward_mean=155.3,reward_threshold=271.3\n",
      "211: loss=0.769, reward_mean=149.5,reward_threshold=269.6\n",
      "212: loss=0.741, reward_mean=141.1,reward_threshold=276.7\n",
      "213: loss=0.737, reward_mean=147.5,reward_threshold=272.1\n",
      "214: loss=0.770, reward_mean=170.4,reward_threshold=266.1\n",
      "215: loss=0.712, reward_mean=129.6,reward_threshold=264.3\n",
      "216: loss=0.757, reward_mean=170.2,reward_threshold=275.0\n",
      "217: loss=0.720, reward_mean=147.1,reward_threshold=267.6\n",
      "218: loss=0.756, reward_mean=136.4,reward_threshold=272.6\n",
      "219: loss=0.719, reward_mean=149.7,reward_threshold=267.1\n",
      "220: loss=0.705, reward_mean=123.7,reward_threshold=272.3\n",
      "221: loss=0.750, reward_mean=137.0,reward_threshold=260.0\n",
      "222: loss=0.736, reward_mean=110.8,reward_threshold=264.6\n",
      "223: loss=0.714, reward_mean=137.1,reward_threshold=265.2\n",
      "224: loss=0.717, reward_mean=134.8,reward_threshold=271.5\n",
      "225: loss=0.685, reward_mean=175.5,reward_threshold=273.1\n",
      "226: loss=0.700, reward_mean=141.5,reward_threshold=274.9\n",
      "227: loss=0.733, reward_mean=128.5,reward_threshold=267.8\n",
      "228: loss=0.723, reward_mean=177.6,reward_threshold=276.6\n",
      "229: loss=0.744, reward_mean=146.4,reward_threshold=277.6\n",
      "230: loss=0.734, reward_mean=151.9,reward_threshold=281.8\n",
      "231: loss=0.734, reward_mean=157.3,reward_threshold=278.5\n",
      "232: loss=0.723, reward_mean=167.2,reward_threshold=280.0\n",
      "233: loss=0.760, reward_mean=161.0,reward_threshold=280.2\n",
      "234: loss=0.736, reward_mean=167.2,reward_threshold=275.6\n",
      "235: loss=0.696, reward_mean=178.9,reward_threshold=282.4\n",
      "236: loss=0.722, reward_mean=181.3,reward_threshold=281.7\n",
      "237: loss=0.732, reward_mean=169.7,reward_threshold=273.3\n",
      "238: loss=0.746, reward_mean=201.4,reward_threshold=276.8\n",
      "Environment has been successfullly completed!\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "\n",
    "\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards),np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f,reward_threshold=%.1f\" % (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/gym/wrappers/monitor.py:31: UserWarning: The Monitor wrapper is being deprecated in favor of gym.wrappers.RecordVideo and gym.wrappers.RecordEpisodeStatistics (see https://github.com/openai/gym/issues/2297)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym.wrappers\n",
    "# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos1\", force=False)\n",
    "generate_batch(env, 1, t_max=500)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
