{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages (from gym) (1.21.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages (from gym) (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: box2d-py in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages (2.3.8)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ! pip install gym\n",
    "# ! pip3 install box2d-py\n",
    "# ! pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz\n",
    "# ! pip install 'ribs[all]' gym~=0.17.0 Box2D~=2.3.10 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=1000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "            new_s, r, done, info = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    " \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "\n",
    "    return elite_states,elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.375, reward_mean=-182.7,reward_threshold=-96.0\n",
      "1: loss=1.374, reward_mean=-188.2,reward_threshold=-100.9\n",
      "2: loss=1.375, reward_mean=-205.0,reward_threshold=-112.9\n",
      "3: loss=1.374, reward_mean=-175.3,reward_threshold=-93.3\n",
      "4: loss=1.376, reward_mean=-158.6,reward_threshold=-93.3\n",
      "5: loss=1.376, reward_mean=-154.8,reward_threshold=-92.4\n",
      "6: loss=1.375, reward_mean=-163.9,reward_threshold=-96.7\n",
      "7: loss=1.373, reward_mean=-152.2,reward_threshold=-92.7\n",
      "8: loss=1.377, reward_mean=-150.7,reward_threshold=-93.9\n",
      "9: loss=1.370, reward_mean=-151.0,reward_threshold=-90.8\n",
      "10: loss=1.371, reward_mean=-153.6,reward_threshold=-89.9\n",
      "11: loss=1.381, reward_mean=-139.4,reward_threshold=-95.9\n",
      "12: loss=1.371, reward_mean=-155.8,reward_threshold=-97.0\n",
      "13: loss=1.376, reward_mean=-136.9,reward_threshold=-84.1\n",
      "14: loss=1.371, reward_mean=-139.0,reward_threshold=-83.0\n",
      "15: loss=1.372, reward_mean=-129.0,reward_threshold=-83.1\n",
      "16: loss=1.365, reward_mean=-151.2,reward_threshold=-80.7\n",
      "17: loss=1.362, reward_mean=-141.8,reward_threshold=-85.8\n",
      "18: loss=1.364, reward_mean=-135.7,reward_threshold=-69.1\n",
      "19: loss=1.356, reward_mean=-171.1,reward_threshold=-91.5\n",
      "20: loss=1.343, reward_mean=-144.6,reward_threshold=-76.3\n",
      "21: loss=1.349, reward_mean=-159.2,reward_threshold=-81.7\n",
      "22: loss=1.335, reward_mean=-151.3,reward_threshold=-80.5\n",
      "23: loss=1.339, reward_mean=-141.8,reward_threshold=-70.3\n",
      "24: loss=1.331, reward_mean=-136.9,reward_threshold=-61.4\n",
      "25: loss=1.324, reward_mean=-138.2,reward_threshold=-62.3\n",
      "26: loss=1.312, reward_mean=-164.0,reward_threshold=-71.0\n",
      "27: loss=1.300, reward_mean=-149.4,reward_threshold=-57.7\n",
      "28: loss=1.297, reward_mean=-153.8,reward_threshold=-53.1\n",
      "29: loss=1.307, reward_mean=-153.1,reward_threshold=-75.0\n",
      "30: loss=1.292, reward_mean=-148.6,reward_threshold=-59.8\n",
      "31: loss=1.290, reward_mean=-147.5,reward_threshold=-47.1\n",
      "32: loss=1.243, reward_mean=-156.6,reward_threshold=-56.4\n",
      "33: loss=1.279, reward_mean=-141.0,reward_threshold=-61.5\n",
      "34: loss=1.277, reward_mean=-124.7,reward_threshold=-52.2\n",
      "35: loss=1.260, reward_mean=-128.0,reward_threshold=-53.8\n",
      "36: loss=1.258, reward_mean=-109.5,reward_threshold=-36.2\n",
      "37: loss=1.260, reward_mean=-132.4,reward_threshold=-48.9\n",
      "38: loss=1.282, reward_mean=-113.7,reward_threshold=-41.8\n",
      "39: loss=1.248, reward_mean=-112.8,reward_threshold=-28.8\n",
      "40: loss=1.256, reward_mean=-93.5,reward_threshold=-22.5\n",
      "41: loss=1.266, reward_mean=-104.8,reward_threshold=-39.9\n",
      "42: loss=1.231, reward_mean=-87.6,reward_threshold=-24.0\n",
      "43: loss=1.212, reward_mean=-93.0,reward_threshold=-13.0\n",
      "44: loss=1.230, reward_mean=-80.7,reward_threshold=-15.8\n",
      "45: loss=1.230, reward_mean=-86.2,reward_threshold=-30.3\n",
      "46: loss=1.237, reward_mean=-93.4,reward_threshold=-18.1\n",
      "47: loss=1.219, reward_mean=-96.4,reward_threshold=-16.7\n",
      "48: loss=1.239, reward_mean=-98.3,reward_threshold=-17.6\n",
      "49: loss=1.171, reward_mean=-78.6,reward_threshold=-10.7\n",
      "50: loss=1.188, reward_mean=-88.9,reward_threshold=-13.6\n",
      "51: loss=1.183, reward_mean=-94.1,reward_threshold=-23.1\n",
      "52: loss=1.204, reward_mean=-91.3,reward_threshold=0.6\n",
      "53: loss=1.190, reward_mean=-92.9,reward_threshold=-14.2\n",
      "54: loss=1.158, reward_mean=-85.1,reward_threshold=-7.0\n",
      "55: loss=1.190, reward_mean=-82.0,reward_threshold=-3.1\n",
      "56: loss=1.191, reward_mean=-74.4,reward_threshold=2.7\n",
      "57: loss=1.170, reward_mean=-99.7,reward_threshold=-2.6\n",
      "58: loss=1.183, reward_mean=-68.3,reward_threshold=15.0\n",
      "59: loss=1.193, reward_mean=-94.0,reward_threshold=3.1\n",
      "60: loss=1.164, reward_mean=-102.9,reward_threshold=-3.9\n",
      "61: loss=1.190, reward_mean=-74.9,reward_threshold=4.2\n",
      "62: loss=1.151, reward_mean=-53.8,reward_threshold=12.5\n",
      "63: loss=1.197, reward_mean=-59.8,reward_threshold=7.0\n",
      "64: loss=1.190, reward_mean=-30.7,reward_threshold=15.0\n",
      "65: loss=1.131, reward_mean=-30.0,reward_threshold=15.6\n",
      "66: loss=1.151, reward_mean=-30.8,reward_threshold=10.6\n",
      "67: loss=1.181, reward_mean=-29.5,reward_threshold=7.0\n",
      "68: loss=1.150, reward_mean=-19.0,reward_threshold=20.0\n",
      "69: loss=1.211, reward_mean=-15.7,reward_threshold=14.1\n",
      "70: loss=1.187, reward_mean=-20.7,reward_threshold=18.9\n",
      "71: loss=1.156, reward_mean=-23.0,reward_threshold=22.9\n",
      "72: loss=1.221, reward_mean=-4.7,reward_threshold=23.6\n",
      "73: loss=1.199, reward_mean=-20.0,reward_threshold=15.3\n",
      "74: loss=1.208, reward_mean=-19.3,reward_threshold=28.2\n",
      "75: loss=1.194, reward_mean=-19.7,reward_threshold=22.2\n",
      "76: loss=1.151, reward_mean=-4.4,reward_threshold=24.8\n",
      "77: loss=1.194, reward_mean=-12.2,reward_threshold=23.0\n",
      "78: loss=1.180, reward_mean=-2.8,reward_threshold=34.4\n",
      "79: loss=1.215, reward_mean=-4.2,reward_threshold=29.3\n",
      "80: loss=1.184, reward_mean=-2.1,reward_threshold=33.3\n",
      "81: loss=1.190, reward_mean=-4.5,reward_threshold=31.6\n",
      "82: loss=1.195, reward_mean=0.6,reward_threshold=35.5\n",
      "83: loss=1.160, reward_mean=4.9,reward_threshold=36.0\n",
      "84: loss=1.208, reward_mean=-13.0,reward_threshold=28.4\n",
      "85: loss=1.193, reward_mean=-12.3,reward_threshold=34.1\n",
      "86: loss=1.205, reward_mean=10.1,reward_threshold=36.4\n",
      "87: loss=1.200, reward_mean=-0.5,reward_threshold=35.7\n",
      "88: loss=1.201, reward_mean=6.3,reward_threshold=32.9\n",
      "89: loss=1.192, reward_mean=-2.9,reward_threshold=42.0\n",
      "90: loss=1.218, reward_mean=-9.4,reward_threshold=34.1\n",
      "91: loss=1.200, reward_mean=2.0,reward_threshold=46.0\n",
      "92: loss=1.199, reward_mean=1.3,reward_threshold=43.7\n",
      "93: loss=1.237, reward_mean=-3.1,reward_threshold=47.4\n",
      "94: loss=1.226, reward_mean=13.2,reward_threshold=44.4\n",
      "95: loss=1.220, reward_mean=14.5,reward_threshold=47.0\n",
      "96: loss=1.228, reward_mean=14.9,reward_threshold=45.4\n",
      "97: loss=1.236, reward_mean=10.3,reward_threshold=44.9\n",
      "98: loss=1.229, reward_mean=9.5,reward_threshold=39.2\n",
      "99: loss=1.243, reward_mean=7.2,reward_threshold=34.9\n",
      "100: loss=1.253, reward_mean=17.6,reward_threshold=49.5\n",
      "101: loss=1.243, reward_mean=8.8,reward_threshold=42.2\n",
      "102: loss=1.239, reward_mean=11.4,reward_threshold=47.5\n",
      "103: loss=1.236, reward_mean=5.4,reward_threshold=40.5\n",
      "104: loss=1.238, reward_mean=15.5,reward_threshold=52.2\n",
      "105: loss=1.244, reward_mean=7.7,reward_threshold=40.8\n",
      "106: loss=1.238, reward_mean=-1.9,reward_threshold=37.5\n",
      "107: loss=1.256, reward_mean=14.0,reward_threshold=68.2\n",
      "108: loss=1.256, reward_mean=5.7,reward_threshold=45.5\n",
      "109: loss=1.253, reward_mean=-0.7,reward_threshold=46.7\n",
      "110: loss=1.244, reward_mean=-14.6,reward_threshold=41.8\n",
      "111: loss=1.256, reward_mean=-2.1,reward_threshold=52.5\n",
      "112: loss=1.261, reward_mean=-9.7,reward_threshold=50.1\n",
      "113: loss=1.248, reward_mean=-3.5,reward_threshold=39.5\n",
      "114: loss=1.261, reward_mean=-4.1,reward_threshold=60.9\n",
      "115: loss=1.260, reward_mean=-7.9,reward_threshold=48.3\n",
      "116: loss=1.269, reward_mean=-5.6,reward_threshold=48.7\n",
      "117: loss=1.263, reward_mean=2.9,reward_threshold=59.6\n",
      "118: loss=1.278, reward_mean=8.7,reward_threshold=68.0\n",
      "119: loss=1.277, reward_mean=15.6,reward_threshold=62.1\n",
      "120: loss=1.288, reward_mean=-3.9,reward_threshold=61.4\n",
      "121: loss=1.286, reward_mean=10.2,reward_threshold=70.7\n",
      "122: loss=1.287, reward_mean=3.4,reward_threshold=69.6\n",
      "123: loss=1.289, reward_mean=11.5,reward_threshold=75.6\n",
      "124: loss=1.283, reward_mean=-7.2,reward_threshold=50.8\n",
      "125: loss=1.296, reward_mean=19.2,reward_threshold=75.1\n",
      "126: loss=1.297, reward_mean=23.5,reward_threshold=81.0\n",
      "127: loss=1.299, reward_mean=27.3,reward_threshold=79.1\n",
      "128: loss=1.293, reward_mean=14.8,reward_threshold=69.6\n",
      "129: loss=1.297, reward_mean=20.6,reward_threshold=86.4\n",
      "130: loss=1.298, reward_mean=29.0,reward_threshold=83.0\n",
      "131: loss=1.293, reward_mean=11.3,reward_threshold=61.3\n",
      "132: loss=1.298, reward_mean=17.7,reward_threshold=77.6\n",
      "133: loss=1.300, reward_mean=15.9,reward_threshold=73.3\n",
      "134: loss=1.296, reward_mean=16.4,reward_threshold=87.3\n",
      "135: loss=1.296, reward_mean=18.7,reward_threshold=89.0\n",
      "136: loss=1.293, reward_mean=8.5,reward_threshold=74.8\n",
      "137: loss=1.291, reward_mean=4.0,reward_threshold=64.8\n",
      "138: loss=1.293, reward_mean=9.7,reward_threshold=85.1\n",
      "139: loss=1.290, reward_mean=16.1,reward_threshold=72.8\n",
      "140: loss=1.284, reward_mean=17.6,reward_threshold=77.4\n",
      "141: loss=1.282, reward_mean=11.3,reward_threshold=71.0\n",
      "142: loss=1.287, reward_mean=16.9,reward_threshold=69.9\n",
      "143: loss=1.277, reward_mean=13.4,reward_threshold=81.5\n",
      "144: loss=1.284, reward_mean=14.3,reward_threshold=78.0\n",
      "145: loss=1.274, reward_mean=9.9,reward_threshold=65.6\n",
      "146: loss=1.274, reward_mean=16.4,reward_threshold=73.2\n",
      "147: loss=1.277, reward_mean=-1.5,reward_threshold=60.3\n",
      "148: loss=1.274, reward_mean=6.3,reward_threshold=68.2\n",
      "149: loss=1.278, reward_mean=13.5,reward_threshold=78.4\n",
      "150: loss=1.280, reward_mean=26.0,reward_threshold=81.1\n",
      "151: loss=1.276, reward_mean=18.6,reward_threshold=83.5\n",
      "152: loss=1.286, reward_mean=20.4,reward_threshold=84.5\n",
      "153: loss=1.283, reward_mean=26.0,reward_threshold=88.5\n",
      "154: loss=1.285, reward_mean=16.1,reward_threshold=74.3\n",
      "155: loss=1.290, reward_mean=17.8,reward_threshold=72.3\n",
      "156: loss=1.293, reward_mean=27.9,reward_threshold=81.9\n",
      "157: loss=1.288, reward_mean=19.6,reward_threshold=97.0\n",
      "158: loss=1.284, reward_mean=26.2,reward_threshold=81.0\n",
      "159: loss=1.288, reward_mean=34.2,reward_threshold=80.7\n",
      "160: loss=1.288, reward_mean=17.1,reward_threshold=86.7\n",
      "161: loss=1.285, reward_mean=12.3,reward_threshold=85.6\n",
      "162: loss=1.294, reward_mean=14.9,reward_threshold=92.1\n",
      "163: loss=1.286, reward_mean=33.3,reward_threshold=96.7\n",
      "164: loss=1.288, reward_mean=17.7,reward_threshold=96.9\n",
      "165: loss=1.289, reward_mean=15.6,reward_threshold=84.7\n",
      "166: loss=1.290, reward_mean=30.0,reward_threshold=86.7\n",
      "167: loss=1.290, reward_mean=28.2,reward_threshold=96.4\n",
      "168: loss=1.283, reward_mean=36.9,reward_threshold=99.0\n",
      "169: loss=1.290, reward_mean=28.0,reward_threshold=91.6\n",
      "170: loss=1.284, reward_mean=19.8,reward_threshold=89.5\n",
      "171: loss=1.286, reward_mean=43.8,reward_threshold=102.3\n",
      "172: loss=1.281, reward_mean=26.9,reward_threshold=94.5\n",
      "173: loss=1.278, reward_mean=23.8,reward_threshold=98.6\n",
      "174: loss=1.273, reward_mean=28.3,reward_threshold=89.5\n",
      "175: loss=1.280, reward_mean=46.3,reward_threshold=100.8\n",
      "176: loss=1.273, reward_mean=33.7,reward_threshold=101.2\n",
      "177: loss=1.270, reward_mean=35.3,reward_threshold=91.8\n",
      "178: loss=1.261, reward_mean=43.5,reward_threshold=113.4\n",
      "179: loss=1.257, reward_mean=46.7,reward_threshold=117.8\n",
      "180: loss=1.261, reward_mean=21.0,reward_threshold=96.1\n",
      "181: loss=1.253, reward_mean=45.8,reward_threshold=109.1\n",
      "182: loss=1.253, reward_mean=32.4,reward_threshold=88.9\n",
      "183: loss=1.241, reward_mean=44.2,reward_threshold=106.4\n",
      "184: loss=1.243, reward_mean=40.9,reward_threshold=95.4\n",
      "185: loss=1.245, reward_mean=41.5,reward_threshold=112.9\n",
      "186: loss=1.238, reward_mean=34.2,reward_threshold=107.3\n",
      "187: loss=1.239, reward_mean=40.9,reward_threshold=109.1\n",
      "188: loss=1.240, reward_mean=47.0,reward_threshold=113.2\n",
      "189: loss=1.241, reward_mean=31.9,reward_threshold=91.8\n",
      "190: loss=1.243, reward_mean=35.0,reward_threshold=100.8\n",
      "191: loss=1.242, reward_mean=45.9,reward_threshold=113.9\n",
      "192: loss=1.242, reward_mean=46.1,reward_threshold=106.4\n",
      "193: loss=1.238, reward_mean=46.7,reward_threshold=133.5\n",
      "194: loss=1.239, reward_mean=52.6,reward_threshold=116.9\n",
      "195: loss=1.236, reward_mean=38.8,reward_threshold=119.1\n",
      "196: loss=1.237, reward_mean=33.8,reward_threshold=103.6\n",
      "197: loss=1.236, reward_mean=41.6,reward_threshold=114.3\n",
      "198: loss=1.235, reward_mean=38.1,reward_threshold=118.7\n",
      "199: loss=1.231, reward_mean=48.5,reward_threshold=125.0\n",
      "200: loss=1.224, reward_mean=29.4,reward_threshold=125.5\n",
      "201: loss=1.225, reward_mean=26.8,reward_threshold=116.5\n",
      "202: loss=1.219, reward_mean=39.2,reward_threshold=117.5\n",
      "203: loss=1.212, reward_mean=41.7,reward_threshold=125.7\n",
      "204: loss=1.215, reward_mean=38.6,reward_threshold=122.8\n",
      "205: loss=1.208, reward_mean=59.5,reward_threshold=139.2\n",
      "206: loss=1.207, reward_mean=48.2,reward_threshold=135.2\n",
      "207: loss=1.205, reward_mean=60.6,reward_threshold=139.2\n",
      "208: loss=1.204, reward_mean=73.2,reward_threshold=147.0\n",
      "209: loss=1.198, reward_mean=47.7,reward_threshold=135.7\n",
      "210: loss=1.193, reward_mean=56.7,reward_threshold=132.8\n",
      "211: loss=1.199, reward_mean=52.4,reward_threshold=134.1\n",
      "212: loss=1.201, reward_mean=46.6,reward_threshold=133.2\n",
      "213: loss=1.184, reward_mean=40.6,reward_threshold=123.1\n",
      "214: loss=1.190, reward_mean=57.2,reward_threshold=136.1\n",
      "215: loss=1.183, reward_mean=43.9,reward_threshold=131.3\n",
      "216: loss=1.174, reward_mean=33.6,reward_threshold=130.0\n",
      "217: loss=1.151, reward_mean=64.1,reward_threshold=139.2\n",
      "218: loss=1.165, reward_mean=47.1,reward_threshold=125.5\n",
      "219: loss=1.149, reward_mean=54.6,reward_threshold=130.0\n",
      "220: loss=1.140, reward_mean=59.7,reward_threshold=149.2\n",
      "221: loss=1.148, reward_mean=61.9,reward_threshold=143.5\n",
      "222: loss=1.143, reward_mean=52.5,reward_threshold=145.3\n",
      "223: loss=1.135, reward_mean=57.2,reward_threshold=144.6\n",
      "224: loss=1.149, reward_mean=37.2,reward_threshold=128.6\n",
      "225: loss=1.136, reward_mean=59.9,reward_threshold=144.0\n",
      "226: loss=1.136, reward_mean=55.7,reward_threshold=139.7\n",
      "227: loss=1.139, reward_mean=58.7,reward_threshold=141.9\n",
      "228: loss=1.133, reward_mean=53.2,reward_threshold=122.9\n",
      "229: loss=1.136, reward_mean=70.7,reward_threshold=142.8\n",
      "230: loss=1.138, reward_mean=55.2,reward_threshold=130.0\n",
      "231: loss=1.138, reward_mean=65.5,reward_threshold=142.1\n",
      "232: loss=1.135, reward_mean=42.6,reward_threshold=123.3\n",
      "233: loss=1.137, reward_mean=67.9,reward_threshold=143.8\n",
      "234: loss=1.133, reward_mean=76.1,reward_threshold=150.8\n",
      "235: loss=1.127, reward_mean=57.4,reward_threshold=143.9\n",
      "236: loss=1.128, reward_mean=77.4,reward_threshold=151.0\n",
      "237: loss=1.123, reward_mean=70.4,reward_threshold=149.8\n",
      "238: loss=1.128, reward_mean=64.4,reward_threshold=142.0\n",
      "239: loss=1.116, reward_mean=75.5,reward_threshold=147.4\n",
      "240: loss=1.117, reward_mean=68.9,reward_threshold=139.8\n",
      "241: loss=1.113, reward_mean=71.0,reward_threshold=147.3\n",
      "242: loss=1.108, reward_mean=65.7,reward_threshold=149.6\n",
      "243: loss=1.116, reward_mean=51.3,reward_threshold=135.8\n",
      "244: loss=1.110, reward_mean=59.1,reward_threshold=152.6\n",
      "245: loss=1.102, reward_mean=63.3,reward_threshold=138.0\n",
      "246: loss=1.102, reward_mean=72.1,reward_threshold=144.6\n",
      "247: loss=1.095, reward_mean=73.9,reward_threshold=154.2\n",
      "248: loss=1.093, reward_mean=73.6,reward_threshold=153.5\n",
      "249: loss=1.091, reward_mean=68.3,reward_threshold=152.3\n",
      "250: loss=1.084, reward_mean=84.2,reward_threshold=156.1\n",
      "251: loss=1.069, reward_mean=78.1,reward_threshold=154.4\n",
      "252: loss=1.069, reward_mean=68.4,reward_threshold=153.1\n",
      "253: loss=1.065, reward_mean=76.3,reward_threshold=148.0\n",
      "254: loss=1.058, reward_mean=71.1,reward_threshold=146.9\n",
      "255: loss=1.051, reward_mean=79.1,reward_threshold=158.0\n",
      "256: loss=1.048, reward_mean=92.4,reward_threshold=167.9\n",
      "257: loss=1.039, reward_mean=72.8,reward_threshold=149.7\n",
      "258: loss=1.025, reward_mean=65.0,reward_threshold=145.0\n",
      "259: loss=1.028, reward_mean=72.4,reward_threshold=150.0\n",
      "260: loss=1.013, reward_mean=87.1,reward_threshold=157.3\n",
      "261: loss=1.007, reward_mean=88.4,reward_threshold=153.8\n",
      "262: loss=0.997, reward_mean=64.8,reward_threshold=150.5\n",
      "263: loss=0.980, reward_mean=75.9,reward_threshold=148.3\n",
      "264: loss=0.970, reward_mean=91.3,reward_threshold=153.4\n",
      "265: loss=0.960, reward_mean=57.5,reward_threshold=145.0\n",
      "266: loss=0.959, reward_mean=69.0,reward_threshold=154.2\n",
      "267: loss=0.950, reward_mean=84.7,reward_threshold=174.1\n",
      "268: loss=0.929, reward_mean=58.0,reward_threshold=153.7\n",
      "269: loss=0.908, reward_mean=94.5,reward_threshold=168.5\n",
      "270: loss=0.902, reward_mean=91.4,reward_threshold=162.0\n",
      "271: loss=0.881, reward_mean=67.4,reward_threshold=163.0\n",
      "272: loss=0.878, reward_mean=84.7,reward_threshold=167.2\n",
      "273: loss=0.862, reward_mean=81.2,reward_threshold=171.7\n",
      "274: loss=0.851, reward_mean=75.1,reward_threshold=165.8\n",
      "275: loss=0.838, reward_mean=81.2,reward_threshold=171.7\n",
      "276: loss=0.834, reward_mean=65.3,reward_threshold=178.1\n",
      "277: loss=0.814, reward_mean=74.2,reward_threshold=184.3\n",
      "278: loss=0.793, reward_mean=99.9,reward_threshold=217.3\n",
      "279: loss=0.780, reward_mean=76.7,reward_threshold=217.4\n",
      "280: loss=0.753, reward_mean=113.0,reward_threshold=258.5\n",
      "281: loss=0.718, reward_mean=116.9,reward_threshold=255.8\n",
      "282: loss=0.710, reward_mean=104.1,reward_threshold=260.3\n",
      "283: loss=0.728, reward_mean=104.6,reward_threshold=256.4\n",
      "284: loss=0.788, reward_mean=123.6,reward_threshold=269.1\n",
      "285: loss=0.778, reward_mean=115.2,reward_threshold=260.8\n",
      "286: loss=0.723, reward_mean=117.3,reward_threshold=265.1\n",
      "287: loss=0.730, reward_mean=119.8,reward_threshold=262.8\n",
      "288: loss=0.777, reward_mean=88.1,reward_threshold=242.0\n",
      "289: loss=0.716, reward_mean=109.9,reward_threshold=260.3\n",
      "290: loss=0.736, reward_mean=130.8,reward_threshold=268.1\n",
      "291: loss=0.719, reward_mean=103.8,reward_threshold=260.0\n",
      "292: loss=0.737, reward_mean=82.4,reward_threshold=257.9\n",
      "293: loss=0.742, reward_mean=114.3,reward_threshold=264.4\n",
      "294: loss=0.723, reward_mean=110.2,reward_threshold=263.4\n",
      "295: loss=0.734, reward_mean=128.5,reward_threshold=261.5\n",
      "296: loss=0.749, reward_mean=107.8,reward_threshold=274.7\n",
      "297: loss=0.741, reward_mean=112.2,reward_threshold=259.4\n",
      "298: loss=0.716, reward_mean=117.5,reward_threshold=272.5\n",
      "299: loss=0.741, reward_mean=124.6,reward_threshold=266.2\n",
      "300: loss=0.715, reward_mean=124.4,reward_threshold=270.1\n",
      "301: loss=0.743, reward_mean=130.2,reward_threshold=267.4\n",
      "302: loss=0.704, reward_mean=138.0,reward_threshold=264.9\n",
      "303: loss=0.714, reward_mean=139.7,reward_threshold=274.4\n",
      "304: loss=0.739, reward_mean=149.1,reward_threshold=272.0\n",
      "305: loss=0.725, reward_mean=137.3,reward_threshold=279.6\n",
      "306: loss=0.723, reward_mean=153.1,reward_threshold=269.9\n",
      "307: loss=0.733, reward_mean=162.7,reward_threshold=270.2\n",
      "308: loss=0.744, reward_mean=167.5,reward_threshold=276.4\n",
      "309: loss=0.736, reward_mean=157.2,reward_threshold=275.3\n",
      "310: loss=0.725, reward_mean=167.9,reward_threshold=273.1\n",
      "311: loss=0.734, reward_mean=176.7,reward_threshold=284.7\n",
      "312: loss=0.745, reward_mean=190.3,reward_threshold=279.3\n",
      "313: loss=0.737, reward_mean=178.3,reward_threshold=270.6\n",
      "314: loss=0.784, reward_mean=205.9,reward_threshold=275.7\n",
      "Environment has been successfullly completed!\n",
      "315: loss=0.716, reward_mean=184.4,reward_threshold=272.4\n",
      "316: loss=0.741, reward_mean=196.1,reward_threshold=270.9\n",
      "317: loss=0.755, reward_mean=172.5,reward_threshold=274.4\n",
      "318: loss=0.766, reward_mean=201.9,reward_threshold=277.6\n",
      "Environment has been successfullly completed!\n",
      "319: loss=0.741, reward_mean=179.8,reward_threshold=271.9\n",
      "320: loss=0.737, reward_mean=196.5,reward_threshold=271.7\n",
      "321: loss=0.711, reward_mean=182.3,reward_threshold=273.2\n",
      "322: loss=0.748, reward_mean=191.2,reward_threshold=280.4\n",
      "323: loss=0.764, reward_mean=193.1,reward_threshold=276.3\n",
      "324: loss=0.760, reward_mean=189.4,reward_threshold=279.5\n",
      "325: loss=0.712, reward_mean=182.6,reward_threshold=285.0\n",
      "326: loss=0.724, reward_mean=168.4,reward_threshold=268.7\n",
      "327: loss=0.745, reward_mean=200.1,reward_threshold=286.3\n",
      "Environment has been successfullly completed!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_18431/1954673757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0melite_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6t/fjr9gh2x3kzfp_x8d04mv8gm0000gn/T/ipykernel_18431/2351948936.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(env, batch_size, t_max)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 500\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "learning_rate = 0.0025\n",
    "completion_score = 200\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards),np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f,reward_threshold=%.1f\" % (i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/gym/wrappers/monitor.py:31: UserWarning: The Monitor wrapper is being deprecated in favor of gym.wrappers.RecordVideo and gym.wrappers.RecordEpisodeStatistics (see https://github.com/openai/gym/issues/2297)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gym.wrappers\n",
    "# env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"),directory=\"videos\", force=False)\n",
    "generate_batch(env, 1, t_max=500)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
